{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7bdf92c6",
   "metadata": {},
   "source": [
    "# Single Fidelity Bayesian Optimization\n",
    "\n",
    "For the model we are using:\n",
    "- [SingleTaskGP](https://botorch.org/api/_modules/botorch/models/gp_regression.html#SingleTaskGP)\n",
    "- [RBFKernel](https://docs.gpytorch.ai/en/latest/kernels.html#gpytorch.kernels.RBFKernel)\n",
    "- [ExactMarginalLogLikelihood]()\n",
    "- [fit_gpytorch_model](https://botorch.org/api/_modules/botorch/fit.html#fit_gpytorch_model)\n",
    "- [fit_gpytorch_torch](https://botorch.org/api/_modules/botorch/optim/fit.html#fit_gpytorch_torch)\n",
    "- [ExpectedImprovement](https://botorch.org/api/_modules/botorch/acquisition/analytic.html#ExpectedImprovement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d4d86ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "\n",
    "from botorch.models import SingleTaskGP\n",
    "from gpytorch.kernels.rbf_kernel import RBFKernel\n",
    "\n",
    "from botorch.acquisition.analytic import ExpectedImprovement\n",
    "from botorch.models.transforms.outcome import Standardize\n",
    "\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch import fit_gpytorch_model\n",
    "###\n",
    "#  fix Cholecky jitter error:\n",
    "#  the line search in the L-BFGS algorithm, used by default, can take some very large steps, \n",
    "#  which in turn causes numerical issues in the solves in the underlying gpytorch model.\n",
    "#\n",
    "#  recommended solution: use an optimizer from torch.optim to fit the model\n",
    "#  see [posted issue here](https://github.com/pytorch/botorch/issues/179#issuecomment-504798767)\n",
    "###\n",
    "from botorch.optim.fit import fit_gpytorch_torch \n",
    "\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "\n",
    "import h5py\n",
    "import pickle\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ddcefda",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26486928",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#  load data: targets and features\n",
    "###\n",
    "normalization = \"normalized\" \n",
    "file = h5py.File(\"./targets_and_{}_features.jld2\".format(normalization), \"r\")\n",
    "\n",
    "# feature matrix\n",
    "X = torch.from_numpy(np.transpose(file[\"X\"][:]))\n",
    "# simulation data (high fidelity)\n",
    "y = torch.from_numpy(np.transpose(file[\"gcmc_y\"][:]))\n",
    "# associated simulation costs\n",
    "cost = torch.from_numpy(np.transpose(file[\"gcmc_elapsed_time\"][:]))\n",
    "\n",
    "# total number of COFs in data set\n",
    "nb_COFs = X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bad98598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw data - \n",
      "\tX: torch.Size([608, 14])\n",
      "\t\ty: torch.Size([608])\n",
      "\t\tcost:  torch.Size([608])\n",
      "\n",
      "Ensure features are normalized - \n",
      "max:\n",
      " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       dtype=torch.float64)\n",
      "min:\n",
      " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64)\n",
      "width:\n",
      " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "#  load data: initializing COFs\n",
    "###\n",
    "init_cof_ids_file = pickle.load(\n",
    "                    open('../search_results/{}/initializing_cof_ids_{}.pkl'.format(normalization, normalization), \n",
    "                         'rb'))\n",
    "\n",
    "init_cof_ids = init_cof_ids_file['init_cof_ids']\n",
    "\n",
    "# total number of BO searches to run = number of initializing sets\n",
    "nb_runs = len(init_cof_ids)\n",
    "\n",
    "###\n",
    "#  print info\n",
    "###\n",
    "print(\"raw data - \\n\\tX:\", X.shape)\n",
    "print(\"\\t\\ty:\", y.shape)\n",
    "print(\"\\t\\tcost: \", cost.shape)    \n",
    "    \n",
    "print(\"\\nEnsure features are normalized - \")\n",
    "print(\"max:\\n\", torch.max(X, 0).values)\n",
    "print(\"min:\\n\", torch.min(X, 0).values)\n",
    "print(\"width:\\n\",torch.max(X, 0).values - torch.min(X, 0).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b08b6adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_unsqueezed = X.unsqueeze(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "392087a9",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee13d487",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f80be35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accumulated cost lags the index of the cost_acquired (iteration COF is identified) by 1\n",
    "def accumulated_cost(cost_acquired):\n",
    "    nb_iters = len(cost_acquired)\n",
    "    accumulated_cost = np.zeros(nb_iters)\n",
    "    accumulated_cost[0] = cost_acquired[0]\n",
    "    for i in range(1, len(cost_acquired)):\n",
    "        accumulated_cost[i] = accumulated_cost[i-1] + cost_acquired[i]\n",
    "    return accumulated_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f6078c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_maxes_acquired(y_acquired):\n",
    "    nb_iters = len(y_acquired)\n",
    "    return [max(y_acquired[:i+1]) for i in range(nb_iters)]      "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0fe57fc",
   "metadata": {},
   "source": [
    "#### Construct Initial Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff4258fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct feature matrix of acquired points\n",
    "def build_X_train(ids_acquired):\n",
    "    return X[ids_acquired, :]\n",
    "\n",
    "# construct output vector for acquired points\n",
    "def build_y_train(ids_acquired):\n",
    "    return y[ids_acquired].unsqueeze(-1)\n",
    "\n",
    "# construct vector to track accumulated cost of acquired points\n",
    "def build_cost(ids_acquired):\n",
    "    return cost[ids_acquired]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19725ffe-d025-47ca-bea1-d78a448f2d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct and fit GP model\n",
    "def construct_and_fit_gp_model(X_train, y_train):      \n",
    "    model = SingleTaskGP(X_train, y_train, outcome_transform=Standardize(m=1), covar_module=RBFKernel())\n",
    "    mll   = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    fit_gpytorch_model(mll, optimizer=fit_gpytorch_torch)\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "235cbe25",
   "metadata": {},
   "source": [
    "#### Bayesian Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86f8e42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_Bayesian_optimization(nb_iterations, initializing_COFs, verbose=False):\n",
    "    assert nb_iterations > len(initializing_COFs)\n",
    "    ###\n",
    "    #  initialize system\n",
    "    ###\n",
    "    # select initial COFs for training data\n",
    "    ids_acquired = initializing_COFs\n",
    "    \n",
    "    ###\n",
    "    #  itterate through remaining budget using BO\n",
    "    ###\n",
    "    for i in range(len(initializing_COFs), nb_iterations):\n",
    "        # construct training data (perform experiments)\n",
    "        X_train = build_X_train(ids_acquired)\n",
    "        y_train = build_y_train(ids_acquired)\n",
    "        \n",
    "        # fit GP surrogate model\n",
    "        model = construct_and_fit_gp_model(X_train, y_train)\n",
    "        \n",
    "        # compute expected improvement acquisition function\n",
    "        acq_fn   = ExpectedImprovement(model, best_f=y_train.max().item())\n",
    "        acq_vals = acq_fn.forward(X.unsqueeze(1))\n",
    "        \n",
    "        # identify COF with highest acquisition value currently *not* in training set\n",
    "        ids_sorted_by_aquisition = acq_vals.argsort(descending=True)\n",
    "        for id_max in ids_sorted_by_aquisition:\n",
    "            if not id_max.item() in ids_acquired:\n",
    "                id_max_acq = id_max.item()\n",
    "                break\n",
    "                \n",
    "        ###\n",
    "        #  acquire this COF\n",
    "        ###\n",
    "        ids_acquired = np.concatenate((ids_acquired, [id_max_acq]))\n",
    "        if verbose:\n",
    "            print(\"BO iteration \", i)\n",
    "            print(\"\\tacquired COF \", id_max_acq)\n",
    "            print(\"\\ty = \", y[id_max_acq].item())\n",
    "    \n",
    "    # check budget constraint is stisfied\n",
    "    assert np.size(ids_acquired) == nb_iterations\n",
    "    assert len(np.unique(ids_acquired)) == nb_iterations\n",
    "    return ids_acquired"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6dd7079d",
   "metadata": {},
   "source": [
    "# Run BO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11298aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_iterations = 150\n",
    "nb_COFs_initialization = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d5d2678",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, initializing_COFs in enumerate(init_cof_ids): \n",
    "    # check the length of each initializing set\n",
    "    assert len(initializing_COFs) == nb_COFs_initialization\n",
    "    print(\"run #: {}\".format(i))\n",
    "    \n",
    "    # start timer for BO run\n",
    "    start_time    = time.time()\n",
    "    ###\n",
    "    #  run BO search\n",
    "    ###\n",
    "    ids_acquired  = run_Bayesian_optimization(nb_iterations, initializing_COFs, verbose=True)\n",
    "    \n",
    "    ###\n",
    "    #  post-run analysis\n",
    "    ###\n",
    "    elapsed_time  = time.time() - start_time\n",
    "    y_acquired    = y[ids_acquired]\n",
    "    y_maxes_acq   = get_y_maxes_acquired(y_acquired.detach().numpy())\n",
    "    cost_acquired = build_cost(ids_acquired)\n",
    "    acc_cost      = accumulated_cost(cost_acquired.detach().numpy())\n",
    "        \n",
    "    BO_iter_top_cof_acquired = np.argmax(y_acquired == max(y))\n",
    "    print(\"iteration we acquire top COF = \", BO_iter_top_cof_acquired.item())\n",
    "    \n",
    "    top_cof_acc_cost = sum(cost_acquired[:BO_iter_top_cof_acquired]).item() \n",
    "    print(\"accumulated cost up to observation of top COF = \", top_cof_acc_cost, \" [min]\")\n",
    "    \n",
    "    ###\n",
    "    #  Store SFBO results\n",
    "    ###\n",
    "    sfbo_res = dict({'ids_acquired': ids_acquired,\n",
    "                     'y_acquired': y_acquired.detach().numpy(),\n",
    "                     'y_max_acquired': y_maxes_acq,\n",
    "                     'cost_acquired': cost_acquired.detach().numpy(),\n",
    "                     'accumulated_cost': acc_cost / 60,\n",
    "                     'nb_COFs_initialization': nb_COFs_initialization,\n",
    "                     'BO_iter_top_cof_acquired': BO_iter_top_cof_acquired.item(),\n",
    "                     'elapsed_time (min)':  elapsed_time / 60\n",
    "                    })\n",
    "\n",
    "    with open('../search_results/{}/sfbo_results/sfbo_results_run_{}.pkl'.format(normalization, i), 'wb') as file:\n",
    "        pickle.dump(sfbo_res, file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e303140",
   "metadata": {},
   "source": [
    "# Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1f13694",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_rs_runs = 1000\n",
    "top_cof_id = np.argmax(y).item()\n",
    "\n",
    "rs_res = dict()\n",
    "rs_res['ids_acquired']     = []\n",
    "rs_res['cost_acquired']    = []\n",
    "rs_res['accumulated_cost'] = []\n",
    "\n",
    "for r in range(nb_rs_runs):\n",
    "    rs_ids_acquired     = np.random.choice(range(nb_COFs), replace=False, size=nb_COFs) #  size=nb_iterations\n",
    "    rs_cost_acquired    = build_cost(rs_ids_acquired)\n",
    "    rs_accumulated_cost = accumulated_cost(rs_cost_acquired)\n",
    "    rs_res['ids_acquired'].append(rs_ids_acquired)\n",
    "    rs_res['cost_acquired'].append(rs_cost_acquired)\n",
    "    rs_res['accumulated_cost'].append(rs_accumulated_cost / 60) # unit: hours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edf6476e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###\n",
    "#  test for duplicate runs\n",
    "###\n",
    "def check_random_for_dupicates(rs_res):\n",
    "    for r in range(nb_rs_runs -1):\n",
    "        for rs_ids_acquired in rs_res['ids_acquired'][r+1:]:\n",
    "            if all(rs_res['ids_acquired'][r] == rs_ids_acquired):\n",
    "                print(\"duplicate run found!\")\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "check_random_for_dupicates(rs_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b96b56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rs(rs_res):\n",
    "    for a in np.random.choice(nb_rs_runs, replace=False, size=7):\n",
    "        # check that the lengths \n",
    "        assert len(rs_res['ids_acquired'][a]) == nb_COFs\n",
    "        assert all([cof_id in range(nb_COFs) for cof_id in rs_res['ids_acquired'][a]])\n",
    "    return\n",
    "\n",
    "test_rs(rs_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e24dc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get y_max acquired up to iteration i for i = 1,2,...\n",
    "def y_max(rs_ids_acquired):\n",
    "    y_max_mu      = np.zeros(nb_iterations)\n",
    "    y_max_sig_bot = np.zeros(nb_iterations)\n",
    "    y_max_sig_top = np.zeros(nb_iterations)\n",
    "    # element i of these will be y max at BO iter i\n",
    "    \n",
    "    for i in range(1, nb_iterations+1):\n",
    "        # max value acquired up to this point\n",
    "        y_maxes = np.array([max(y[rs_ids_acquired[r][:i]]) for r in range(nb_rs_runs)])\n",
    "        assert np.size(y_maxes) == nb_rs_runs\n",
    "        y_max_mu[i-1]      = np.mean(y_maxes)\n",
    "        y_max_sig_bot[i-1] = np.std(y_maxes[y_maxes < y_max_mu[i-1]])\n",
    "        y_max_sig_top[i-1] = np.std(y_maxes[y_maxes > y_max_mu[i-1]])\n",
    "    return y_max_mu, y_max_sig_bot, y_max_sig_top\n",
    "\n",
    "# rs_mean, rs_lower_bound, rs_upper_bound = y_max(rs_res)\n",
    "y_rs_max_mu, y_rs_max_sig_bot, y_rs_max_sig_top = y_max(rs_res['ids_acquired'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10aca7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#  Store Random Search Results\n",
    "###\n",
    "random_search_results = dict({'ids_acquired': rs_res['ids_acquired'],\n",
    "                             'y_rs_max_mu': y_rs_max_mu,\n",
    "                             'y_rs_max_sig_bot': y_rs_max_sig_bot,\n",
    "                             'y_rs_max_sig_top': y_rs_max_sig_top,\n",
    "                             'cost_acquired': rs_res['cost_acquired'],\n",
    "                             'accumulated_cost': rs_res['accumulated_cost']\n",
    "                             })\n",
    "\n",
    "with open('../search_results/{}/random_search_results.pkl'.format(normalization), 'wb') as file:\n",
    "    pickle.dump(random_search_results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b72fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
