{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d087dd2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Multi-fidelity Bayesian optimization (MFBO) of COFs for Xe/Kr separations.\n",
    "1. We have a set of COFs from a database. Each COF is characterized by a feature vector $$x \\in \\mathcal{X} \\subset R^d$$ were d=14.\n",
    "\n",
    "2. We have **two different types** of simulations to calculate **the same material property**, the adsorptive Xe/Kr selectivity $S_{Xe/Kr}$. However, we only have a single objective: to maximize the high-fidelity selectivity. \n",
    "$$\\arg\\max_{x \\in \\mathcal{X}}[S^{(\\ell=\\text{high})}_{Xe/Kr}(x)]$$\n",
    "\n",
    "3. Multi-Fidelity options: \n",
    "    1. low-fidelity  => Henry coefficient calculation - MC integration: $S_{Xe/Kr}^{\\text{low}} = \\frac{H_{Xe}}{H_{Kr}}$\n",
    "    2. high-fidelity => GCMC mixture simulation - 80:20 (Kr:Xe) at 298 K and 1.0 bar: $S_{Xe/Kr}^{\\text{high}} = \\frac{n_{Xe} / n_{Kr}}{y_{Xe}/y_{Kr}}$\n",
    "\n",
    "\n",
    "3. We will initialize the surrogate model with a few (3) COFs with simulations under **both** fidelities.\n",
    "    1. The fist COF will be the one closest to the center of the normalized feature space\n",
    "    2. The rest will be chosen to maximize diversity of the training set\n",
    "\n",
    "\n",
    "4. Model:\n",
    "    1. Botorch GP surrogate model: [SingleTaskMultiFidelityGP](https://botorch.org/api/models.html#module-botorch.models.gp_regression_fidelity) or [FixedNoiseMultiFidelityGP](https://botorch.org/api/models.html#botorch.models.gp_regression_fidelity.FixedNoiseMultiFidelityGP)\n",
    "        - Needed to use [this](https://botorch.org/api/optim.html#module-botorch.optim.fit) optimizer to correct matrix jitter\n",
    "    2. We  use the augmented-EI (aEI) acquisition function from [here](https://link.springer.com/content/pdf/10.1007/s00158-005-0587-0.pdf)\n",
    "\n",
    "\n",
    "-  Helpful [tutorial](https://botorch.org/tutorials/discrete_multi_fidelity_bo) for a similar BoTorch Model used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "669c708a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cokes/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "\n",
    "from botorch.models import SingleTaskMultiFidelityGP\n",
    "\n",
    "from botorch.models.transforms.outcome import Standardize\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch import fit_gpytorch_model\n",
    "###\n",
    "#  fix Cholecky jitter error:\n",
    "#  the line search in the L-BFGS algorithm, used by default, can take some very large steps, \n",
    "#  which in turn causes numerical issues in the solves in the underlying gpytorch model.\n",
    "#\n",
    "#  recommended solution: use an optimizer from torch.optim to fit the model\n",
    "#  see [posted issue here](https://github.com/pytorch/botorch/issues/179#issuecomment-504798767)\n",
    "###\n",
    "from botorch.optim.fit import fit_gpytorch_torch \n",
    "\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "import pickle\n",
    "import h5py # for .jld2 files\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "018f4355-0bd1-430d-9c4a-585c4ea7c108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###\n",
    "#  figure settings \n",
    "###\n",
    "import seaborn as sns\n",
    "sns.set(style='ticks', palette='Set2', font_scale=1.5, rc={\"lines.linewidth\": 3})\n",
    "sns.despine()\n",
    "# plt.rcParams.update({'font.size': 16})\n",
    "# plt.rcParams['figure.dpi'] = 1200 # 600-1200 for paper quality\n",
    "\n",
    "save_figures = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887fb687",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb2c587c-20a0-4e3a-9424-f4f42d727c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation_study_flag = False # make features have no information for a baseline. to gauge feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefe6549-4658-456a-8b36-c0266e71a891",
   "metadata": {},
   "source": [
    "first, the targets (simulated adsorption) and features of the COFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a2740b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = h5py.File(\"./targets_and_raw_features.jld2\", \"r\")\n",
    "\n",
    "xtal_names = file['COFs'][:]\n",
    "\n",
    "feature_names = file['feature_names'][:]\n",
    "feature_names = [fn.decode() for fn in feature_names]\n",
    "\n",
    "# feature matrix\n",
    "X = torch.from_numpy(np.transpose(file[\"X\"][:]))\n",
    "\n",
    "if ablation_study_flag:\n",
    "    # shuffle columns\n",
    "    for j in range(X.size()[1]):\n",
    "        shuffled_row_ids = torch.randperm(X.size()[0])\n",
    "        X[:, j] = X[shuffled_row_ids, j]\n",
    "\n",
    "# simulation data\n",
    "y = [torch.from_numpy(np.transpose(file[\"henry_y\"][:])), \n",
    "     torch.from_numpy(np.transpose(file[\"gcmc_y\"][:]))]  \n",
    "\n",
    "# associated simulation costs\n",
    "cost = [np.transpose(file[\"henry_total_elapsed_time\"][:]), # [min]\n",
    "        np.transpose(file[\"gcmc_elapsed_time\"][:])]        # [min]\n",
    "\n",
    "# total number of COFs in data set\n",
    "nb_COFs = X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f19044-cd28-4237-9c8e-51aeab842780",
   "metadata": {},
   "source": [
    "second, the COFs to initialize the surrogate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9f38094-06f5-494f-b61e-8ead0fc3b757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_cof_ids_file = pickle.load(open('../search_results/initializing_cof_ids_normalized.pkl', 'rb'))\n",
    "\n",
    "init_cof_ids = init_cof_ids_file['init_cof_ids']\n",
    "\n",
    "# total number of BO searches to run = number of initializing sets\n",
    "nb_runs = len(init_cof_ids)\n",
    "nb_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693a109d-4ad5-4ee1-a586-dd01083d6b8e",
   "metadata": {},
   "source": [
    "some tests on the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a9edb9b-9ca3-4b04-aa63-4ce2637a9f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: pore_diameter_Å\n",
      "1: void_fraction\n",
      "2: surface_area_m²g⁻¹\n",
      "3: crystal_density\n",
      "4: B\n",
      "5: O\n",
      "6: C\n",
      "7: H\n",
      "8: Si\n",
      "9: N\n",
      "10: S\n",
      "11: P\n",
      "12: halogens\n",
      "13: metals\n"
     ]
    }
   ],
   "source": [
    "for i, f in enumerate(feature_names):\n",
    "    print(\"{}: {}\".format(i, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "79bbe7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_COF = b'19060N2_ddec.cif' # random COF\n",
    "\n",
    "id_rnd_COF = np.where(xtal_names == rnd_COF)[0]\n",
    "\n",
    "# does the low-fidelity selectivity match that manually read from the simulation output file?\n",
    "assert np.isclose(y[0][id_rnd_COF].item(), 722.409 / 202.085)\n",
    "\n",
    "# does the high-fidelity selectivity match that manually read from the simulation output file?\n",
    "assert np.isclose(y[1][id_rnd_COF].item(), (6.1558810248879325 / 6.842906773660418) / (20/80))\n",
    "\n",
    "# manually check some features\n",
    "if not ablation_study_flag:\n",
    "    # ASA_m^2/g = 4363.81 in Zeo++ output file. this better match!\n",
    "    assert np.isclose(X[id_rnd_COF, 2].item(), 4363.81)\n",
    "\n",
    "    # mol fraction of N = 0.04807692307692308 in the xtal. this better match!\n",
    "    assert np.isclose(X[id_rnd_COF, 9].item(), 0.04807692307692308)\n",
    "    \n",
    "    # sum of mol frac's = 1\n",
    "    assert X[id_rnd_COF, 4:].sum().item() == 1\n",
    "    \n",
    "    # pore diameter = 15.12574 from Zeo++ output file\n",
    "    assert X[id_rnd_COF, 0] == 15.12574\n",
    "    \n",
    "    # void fraction = 0.58554 from Zeo++ output file.\n",
    "    assert X[id_rnd_COF, 1] == 0.58554\n",
    "    \n",
    "    # xtal density from Zeo++ output = 0.604869\n",
    "    assert np.isclose(X[id_rnd_COF, 3], 0.604869 * 1000, atol=0.1) # unit convert cuz computed in PM.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cd08d5-42b8-4fe2-9f89-86c3f61fe0f9",
   "metadata": {},
   "source": [
    "now that we've tested, normalize the features to lie in [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "99dece51-34b9-4d92-a774-bcdab554bedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(X.size()[1]):\n",
    "    X[:, j] = (X[:, j] - torch.min(X[:, j]).item()) / (torch.max(X[:, j]).item() - torch.min(X[:, j]).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f52c720-2f53-409b-a315-e0442d59950d",
   "metadata": {},
   "source": [
    "print stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "54e94e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total high-fidelity cost: 139887.66223703226 [min]\n",
      "total low-fidelity cost:  10076.305239888028 [min]\n",
      "average high-fidelity cost: 230.0783918372241 [min]\n",
      "average low-fidelity cost:  16.57287046034216 [min]\n",
      "average cost ratio:\t    13.444745568580501\n",
      "\n",
      "raw data - \n",
      "\tX: torch.Size([608, 14])\n",
      "\tfidelity: 0\n",
      "\t\ty: torch.Size([608])\n",
      "\t\tcost:  (608,)\n",
      "\tfidelity: 1\n",
      "\t\ty: torch.Size([608])\n",
      "\t\tcost:  (608,)\n",
      "\n",
      "Ensure features are normalized - \n",
      "max:\n",
      " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       dtype=torch.float64)\n",
      "min:\n",
      " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64)\n",
      "width:\n",
      " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       dtype=torch.float64)\n",
      "mean:\n",
      " tensor([0.2856, 0.5864, 0.5304, 0.3323, 0.0421, 0.1617, 0.6405, 0.6793, 0.0062,\n",
      "        0.1758, 0.0308, 0.0131, 0.0132, 0.0238], dtype=torch.float64)\n",
      "std:\n",
      " tensor([0.1586, 0.1787, 0.1896, 0.1512, 0.1150, 0.1916, 0.1631, 0.1354, 0.0631,\n",
      "        0.1126, 0.1026, 0.0935, 0.0740, 0.1043], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# cost\n",
    "print(\"total high-fidelity cost:\", sum(cost[1]).item(), \"[min]\")\n",
    "print(\"total low-fidelity cost: \", sum(cost[0]).item(), \"[min]\")\n",
    "print(\"average high-fidelity cost:\", np.mean(cost[1]), \"[min]\")\n",
    "print(\"average low-fidelity cost: \", np.mean(cost[0]), \"[min]\")\n",
    "print(\"average cost ratio:\\t   \", np.mean(cost[1] / cost[0]))\n",
    "\n",
    "# data shape\n",
    "print(\"\\nraw data - \\n\\tX:\", X.shape)\n",
    "for f in range(2):\n",
    "    print(\"\\tfidelity:\", f)\n",
    "    print(\"\\t\\ty:\", y[f].shape)\n",
    "    print(\"\\t\\tcost: \", cost[f].shape)\n",
    "    \n",
    "# normalization check\n",
    "print(\"\\nEnsure features are normalized - \")\n",
    "print(\"max:\\n\", torch.max(X, 0).values)\n",
    "print(\"min:\\n\", torch.min(X, 0).values)\n",
    "print(\"width:\\n\",torch.max(X, 0).values - torch.min(X, 0).values)\n",
    "print(\"mean:\\n\", torch.mean(X, 0))\n",
    "print(\"std:\\n\", torch.std(X, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f9ab59",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfab7a99",
   "metadata": {},
   "source": [
    "#### Post-Search Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7fb35b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_fidelity_fraction(acquired_set, fidelity):\n",
    "    assert fidelity in [1/3, 2/3] \n",
    "    nb_iters = len(acquired_set)\n",
    "    fid_frac = np.zeros(nb_iters)\n",
    "    for i in range(1, nb_iters):\n",
    "        fid_frac[i] = sum(acquired_set[:, 0][:i+1] == fidelity) / (i+1)\n",
    "    return fid_frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a8936ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# very crude way to get fidelity id\n",
    "# exploits lf < 0.5 and hf > 0.5\n",
    "# reurns a 0 or 1\n",
    "def get_f_ids(acquired_set):\n",
    "    if acquired_set.dim() == 0:\n",
    "        return acquired_set.round().to(dtype=int)\n",
    "    else: \n",
    "        f_ids = [a[0].round().to(dtype=int) for a in acquired_set]\n",
    "        return torch.tensor(f_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c591a6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_maxes_acquired(acquired_set):    \n",
    "    nb_iters = len(acquired_set)\n",
    "    y_maxes = np.zeros(nb_iters)\n",
    "    # we want the maximum y value (only high-fidelity) up to a given iteration\n",
    "    y_max = 0.0 # update this each iteration.\n",
    "    for i, (f_val, cof_id) in enumerate(acquired_set):\n",
    "        f_id = get_f_ids(torch.tensor(f_val))\n",
    "        assert f_id in [0, 1]\n",
    "        y_acq_this_iter = y[f_id][int(cof_id)]\n",
    "        # i is iteration index\n",
    "        if f_id == 1 and y_acq_this_iter > y_max:  \n",
    "            y_max = y_acq_this_iter # over-write max\n",
    "        y_maxes[i] = y_max \n",
    "    return y_maxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8b3d16cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulated_cost(cost_acquired):\n",
    "    nb_iters = len(acquired_set)\n",
    "    accumulated_cost = np.zeros(nb_iters)\n",
    "    accumulated_cost[0] = cost_acquired[0]\n",
    "    for i in range(1, len(cost_acquired)):\n",
    "        accumulated_cost[i] = accumulated_cost[i-1] + cost_acquired[i]\n",
    "    return accumulated_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23cc873",
   "metadata": {},
   "source": [
    "#### Construct Initial Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1f187575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_acquired_set(X, y, initializing_COFs, discrete_fidelities):\n",
    "#     cof_ids = diverse_set(X, nb_COFs_initialization) # np.array(ids_train)\n",
    "    return torch.tensor([[f_id, cof_id] for cof_id in initializing_COFs for f_id in discrete_fidelities])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f66725a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct feature matrix of acquired points\n",
    "def build_X_train(acquired_set):\n",
    "    cof_ids = [a[1].to(dtype=int) for a in acquired_set]\n",
    "    f_ids = torch.tensor([a[0] for a in acquired_set])\n",
    "    assert f_ids[0] in [1/3, 2/3]\n",
    "    return torch.cat((X[cof_ids, :], f_ids.unsqueeze(dim=-1)), dim=1)\n",
    "\n",
    "# construct output vector for acquired points\n",
    "def build_y_train(acquired_set):\n",
    "    f_ids = get_f_ids(acquired_set)\n",
    "    cof_ids = [a[1].to(dtype=int) for a in acquired_set]\n",
    "    return torch.tensor([y[f_id][cof_id] for f_id, cof_id in zip(f_ids, cof_ids)]).unsqueeze(-1)\n",
    "\n",
    "# construct vector to track accumulated cost of acquired points\n",
    "def build_cost(acquired_set):\n",
    "    f_ids = get_f_ids(acquired_set)\n",
    "    cof_ids = [a[1].to(dtype=int) for a in acquired_set]\n",
    "    return torch.tensor([cost[f_id][cof_id] for f_id, cof_id in zip(f_ids, cof_ids)]).unsqueeze(-1)\n",
    "\n",
    "# construct vector to track accumulated cost of acquired points\n",
    "def build_cost_fidelity(acquired_set, fidelity):\n",
    "    f_ids = get_f_ids(acquired_set)\n",
    "    cof_ids = [a[1].to(dtype=int) for a in acquired_set]\n",
    "    return torch.tensor([cost[f_id][cof_id] for f_id, cof_id in zip(f_ids, cof_ids) if f_id == fidelity]).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec40fbc",
   "metadata": {},
   "source": [
    "some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8d1bb88d-f2e7-4850-9b08-0113135d7353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(X[[10, 3, 4], :], bogus_X_train[:, :14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3b0baae2-e954-4cac-8fd6-6949ec5d1526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5bd92850-307a-4342-995b-f2be7993916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bogus_acquired_set = torch.tensor([[2/3, 10], [1/3, 3], [1/3, 4]])\n",
    "\n",
    "###\n",
    "#   build_X_train\n",
    "###\n",
    "bogus_X_train = build_X_train(bogus_acquired_set)\n",
    "# first 14 are features.\n",
    "np.allclose(X[[10, 3, 4], :], bogus_X_train[:, :14])\n",
    "\n",
    "# last one is fidelity param\n",
    "assert np.allclose(bogus_X_train[:, 14], [2/3, 1/3, 1/3])\n",
    "\n",
    "###\n",
    "#   build_y_train\n",
    "###\n",
    "bogus_y_train = build_y_train(bogus_acquired_set)\n",
    "assert bogus_y_train[0] == y[1][10] # y[fid_id][cof_id]\n",
    "assert bogus_y_train[1] == y[0][3]\n",
    "assert bogus_y_train[2] == y[0][4]\n",
    "#bogus_X_train[0, :14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bda9e756-ecaf-4b66-a568-58fdf01a5bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0241, 0.0959, 0.4220, 0.6899, 0.2895, 0.4211, 0.2811, 0.8421, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6667],\n",
       "        [0.0815, 0.2693, 0.1253, 0.6615, 0.3929, 0.5714, 0.7213, 0.4286, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333],\n",
       "        [0.2144, 0.5197, 0.3039, 0.3981, 0.2292, 0.3333, 0.7561, 0.5833, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333]], dtype=torch.float64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    assert y_train[0] == y[1][10] # y[fid_id][cof_id]\n",
    "    assert y_train[2] == y[0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7f1f9678",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'acquired_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m y_train[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m==\u001b[39m y[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m4\u001b[39m]\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[43mtest_initializing_functions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[71], line 8\u001b[0m, in \u001b[0;36mtest_initializing_functions\u001b[0;34m(X, y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_initializing_functions\u001b[39m(X, y):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m###\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m#  construct training sets\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Training Sets\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m build_X_train(\u001b[43macquired_set\u001b[49m)\n\u001b[1;32m      9\u001b[0m     y_train \u001b[38;5;241m=\u001b[39m build_y_train(acquired_set)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m###\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m#  test that the constructor functions are working properly\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m###\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'acquired_set' is not defined"
     ]
    }
   ],
   "source": [
    "def test_initializing_functions(X, y):\n",
    "    ###\n",
    "    #  construct training sets\n",
    "    ###\n",
    "    # list of (cof_id, fid_id)'s\n",
    "    \n",
    "    # Training Sets\n",
    "    X_train = build_X_train(acquired_set)\n",
    "    y_train = build_y_train(acquired_set)\n",
    "    \n",
    "    ###\n",
    "    #  test that the constructor functions are working properly\n",
    "    ###\n",
    "    assert np.allclose(X[10, :], X_train[0, :14])\n",
    "    assert np.isclose(X_train[0, 14], 2/3) # high-fidelity\n",
    "    assert np.isclose(X_train[1, 14], 1/3) # low-fidelity\n",
    "    assert y_train[0] == y[1][10] # y[fid_id][cof_id]\n",
    "    assert y_train[2] == y[0][4]\n",
    "    return\n",
    "\n",
    "test_initializing_functions(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d2ef9b",
   "metadata": {},
   "source": [
    "### Surrogate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b21c9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_surrogate_model(X_train, y_train):\n",
    "    model = SingleTaskMultiFidelityGP(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        linear_truncated=False, # RBF for features and Downsampling for Fidelities\n",
    "        outcome_transform=Standardize(m=1), # m is the output dimension\n",
    "        data_fidelity=X_train.shape[1] - 1\n",
    "    )   \n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    fit_gpytorch_model(mll, optimizer=fit_gpytorch_torch)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a87aeb",
   "metadata": {},
   "source": [
    "### Acquisition Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6672eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate posterior mean and variance for a given fidelity\n",
    "def mu_sigma(model, X, fidelity):\n",
    "    assert fidelity in [1/3, 2/3]\n",
    "    nb_COFs_here = X.size()[0]\n",
    "    f = torch.tensor((), dtype=torch.float64).new_ones((nb_COFs_here, 1)) * fidelity\n",
    "    X_f = torch.cat((X, f), dim=1) # last col is associated fidelity\n",
    "    f_posterior = model.posterior(X_f)\n",
    "    return f_posterior.mean.squeeze().detach().numpy(), np.sqrt(f_posterior.variance.squeeze().detach().numpy())\n",
    "\n",
    "# get the current best y-value of desired_fidelity in the acquired set\n",
    "def get_y_max(acquired_set, desired_fidelity):\n",
    "    assert desired_fidelity in [0, 1]\n",
    "    f_ids = get_f_ids(acquired_set)\n",
    "    cof_ids = [a[1].to(dtype=int) for a in acquired_set]\n",
    "    return np.max([y[f_id][cof_id] for f_id, cof_id in zip(f_ids, cof_ids) if f_id == desired_fidelity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccf3d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#  efficient multi-fidelity correlation function\n",
    "#  corr(y at given fidelity, y at high-fidelity)\n",
    "#  (see notes)\n",
    "###\n",
    "def mfbo_correlation_function(model, X, fidelity):\n",
    "    assert fidelity in [1/3, 2/3]\n",
    "    # given fidelity\n",
    "    f   = torch.tensor((), dtype=torch.float64).new_ones((nb_COFs, 1)) * fidelity\n",
    "    X_f = torch.cat((X, f), dim=1) # last col is associated fidelity\n",
    "    \n",
    "    #  high-fidelity\n",
    "    hf   = torch.tensor((), dtype=torch.float64).new_ones((nb_COFs, 1)) * discrete_fidelities[-1]\n",
    "    X_hf = torch.cat((X, hf), dim=1) # last col is associated fidelity\n",
    "\n",
    "    # combine into a single tensor\n",
    "    X_all_fid = torch.cat((X_f, X_hf), dim=0)\n",
    "    \n",
    "    # get variance for each fidelity\n",
    "    var_f = torch.flatten(model.posterior(X_f).variance)\n",
    "    var_hf = torch.flatten(model.posterior(X_hf).variance) # variance\n",
    "    \n",
    "    # posterior covariance \n",
    "    cov = torch.diag(model(X_all_fid).covariance_matrix[:X_f.size()[0], X_f.size()[0]:])\n",
    "    \n",
    "    corr = cov / (torch.sqrt(var_f) * torch.sqrt(var_hf))\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b278ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#  cost ratio\n",
    "###\n",
    "def estimate_cost_ratio(fidelity, acquired_set):\n",
    "    assert fidelity in [1/3, 2/3]\n",
    "    f_id = get_f_ids(torch.tensor(fidelity))\n",
    "    avg_cost_f  = torch.mean(build_cost_fidelity(acquired_set, f_id))\n",
    "    avg_cost_hf = torch.mean(build_cost_fidelity(acquired_set, 1))\n",
    "    cr = avg_cost_hf / avg_cost_f\n",
    "    return cr.item()\n",
    "\n",
    "###\n",
    "#  expected imrovement function, only uses hf\n",
    "###\n",
    "def EI_hf(model, X, acquired_set):\n",
    "    hf_mu, hf_sigma = mu_sigma(model, X, discrete_fidelities[-1])\n",
    "    y_max = get_y_max(acquired_set, 1)\n",
    "    \n",
    "    z = (hf_mu - y_max) / hf_sigma\n",
    "    explore_term = hf_sigma * norm.pdf(z) \n",
    "    exploit_term = (hf_mu - y_max) * norm.cdf(z) \n",
    "    ei = explore_term + exploit_term\n",
    "    return np.maximum(ei, np.zeros(nb_COFs))\n",
    "\n",
    "###\n",
    "#  acquisition function\n",
    "###\n",
    "def acquisition_scores(model, X, fidelity, acquired_set):\n",
    "    assert fidelity in [1/3, 2/3]\n",
    "    # expected improvement for high-fidelity\n",
    "    ei = EI_hf(model, X, acquired_set) \n",
    "    \n",
    "    # augmenting functions\n",
    "    corr_f1_f0 = mfbo_correlation_function(model, X, fidelity)\n",
    "    \n",
    "    cr = estimate_cost_ratio(fidelity, acquired_set)\n",
    "\n",
    "    scores = torch.from_numpy(ei) * corr_f1_f0 * cr\n",
    "    return scores.detach().numpy()\n",
    "\n",
    "def in_acquired_set(f_id, cof_id, acquired_set):\n",
    "    assert f_id in [0, 1]\n",
    "    fidelity = discrete_fidelities[f_id]\n",
    "    for this_fidelity, this_cof_id in acquired_set:\n",
    "        if this_cof_id == cof_id and this_fidelity == fidelity:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b84cbe",
   "metadata": {},
   "source": [
    "### Bayesian Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae82c1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_Bayesian_optimization(nb_iterations, initializing_COFs, verbose=False):\n",
    "    assert nb_iterations > len(initializing_COFs)\n",
    "    ###\n",
    "    #  initialize system\n",
    "    ###\n",
    "    acquired_set = initialize_acquired_set(X, y, initializing_COFs, discrete_fidelities)\n",
    "    \n",
    "    ###\n",
    "    #  itterate through remaining budget using BO\n",
    "    ###\n",
    "    for i in range(nb_COFs_initialization * len(discrete_fidelities), nb_iterations): \n",
    "        print(\"BO iteration: \", i)\n",
    "        ###\n",
    "        #  construct training data (perform experiments)\n",
    "        ###\n",
    "        X_train = build_X_train(acquired_set)\n",
    "        y_train = build_y_train(acquired_set)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Initialization - \\n\")\n",
    "            print(\"\\tid acquired = \", [acq_[1].item() for acq_ in acquired_set])\n",
    "            print(\"\\tfidelity acquired = \", [acq_[0].item() for acq_ in acquired_set])\n",
    "            print(\"\\tcosts acquired = \", build_cost(acquired_set), \" [min]\")\n",
    "\n",
    "            print(\"\\n\\tTraining data:\\n\")\n",
    "            print(\"\\t\\t X train shape = \", X_train.shape)\n",
    "            print(\"\\t\\t y train shape = \", y_train.shape)\n",
    "            print(\"\\t\\t training feature vector = \\n\", X_train)\n",
    "        \n",
    "        ###\n",
    "        #  train Model\n",
    "        ###\n",
    "        model = train_surrogate_model(X_train, y_train)\n",
    "        \n",
    "        ###\n",
    "        #  acquire new (COF, fidelity) not yet acquired.\n",
    "        ###\n",
    "        # entry (fid_id, cof_id) is the acquisition value for fidelity f_id and cof cof_id\n",
    "        the_acquisition_scores = np.array([acquisition_scores(model, X, fidelity, acquired_set) for fidelity in discrete_fidelities])\n",
    "        \n",
    "        # overwrite acquired COFs/fidelities with negative infinity to not choose these.\n",
    "        for fidelity, cof_id in acquired_set:\n",
    "            the_acquisition_scores[get_f_ids(fidelity), cof_id.to(dtype=int)] = - np.inf\n",
    "        \n",
    "        # select COF/fidelity with highest aquisition score.\n",
    "        f_id, cof_id = np.unravel_index(np.argmax(the_acquisition_scores), np.shape(the_acquisition_scores))\n",
    "        assert f_id in [0, 1]\n",
    "        assert not in_acquired_set(f_id, cof_id, acquired_set)\n",
    "        \n",
    "        # update acquired_set\n",
    "        acq = torch.tensor([[discrete_fidelities[f_id], cof_id]]) # dtype=int\n",
    "        print(acq)\n",
    "        acquired_set = torch.cat((acquired_set, acq))\n",
    "\n",
    "        ###\n",
    "        #  print useful info\n",
    "        ###\n",
    "        if verbose:\n",
    "            print(\"\\tacquired COF \", cof_id, \" at fidelity, \", f_id)\n",
    "            print(\"\\t\\ty = \", y[f_id][cof_id].item())\n",
    "            print(\"\\t\\tcost = \", cost[f_id][cof_id])\n",
    "        \n",
    "    return acquired_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2130854c",
   "metadata": {},
   "source": [
    "# Run MFBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c4d0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#  construct initial inputs\n",
    "###\n",
    "discrete_fidelities = [1/3, 2/3] # set of discrete fidelities (in ascending order) to select from\n",
    "\n",
    "nb_COFs_initialization = 3   # at each fidelity, number of COFs to initialize with\n",
    "nb_iterations = 150          # BO budget, includes initializing COFs\n",
    "\n",
    "\n",
    "if ablation_study_flag:\n",
    "    print(\"ablation study: {}\".format(ablation_study_flag))\n",
    "    # the maximum possible number itterations = num_fidelities * nb_COFs\n",
    "    # this would efectively constitute a low-fidelity exhaustive search \n",
    "    # followed by a high-fidelity exhaustive search\n",
    "    nb_iterations = 2 * nb_COFs \n",
    "    print(\"max. number of itterations: {}\".format(nb_iterations))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f65bd87",
   "metadata": {},
   "source": [
    "**Using the set with COF closest to the center of feature space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdf80b8-f37d-4001-8b48-ee4495539fba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ids_acquired = run_Bayesian_optimization(nb_iterations, init_cof_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b271083c-c9f4-4780-823f-c77ff3791b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack search results\n",
    "fids = [ids_acquired[i][0].item() for i in range(len(ids_acquired))]\n",
    "cof_ids = [int(ids_acquired[i][1].item()) for i in range(len(ids_acquired))]\n",
    "\n",
    "# which COF has the largest high-fidelity selectivity?\n",
    "cof_id_with_max_hi_fid_selectivity = np.argmax(y[1]).item()\n",
    "\n",
    "# iteration we found top COF\n",
    "n_iter_top_cof_found = np.where([cof_ids[i] == cof_id_with_max_hi_fid_selectivity and fids[i] > 0.5 for i in range(len(cof_ids))])[0].item()\n",
    "n_iter_top_cof_found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7ded64-5ad7-4671-9c1b-6d04d568110e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### observe status of surrogate model's knowledge the iteration before the top COF was acquired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce31cd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the selectivities of COFs according to low vs high fid\n",
    "# which COFs are sampled at both fidelities?\n",
    "hi_fid_cofs = [cof_ids[i] for i in range(n_iter_top_cof_found) if fids[i] > 0.5]\n",
    "lo_fid_cofs = [cof_ids[i] for i in range(n_iter_top_cof_found) if fids[i] < 0.5]\n",
    "ids_cofs_hi_and_lo_fid = np.intersect1d(hi_fid_cofs, lo_fid_cofs)\n",
    "ids_cofs_hi_and_lo_fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7415a9f-fac3-4beb-ae9b-bab536ddc639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build selectivity array for plotting\n",
    "y_los = [y[0][c].item() for c in ids_cofs_hi_and_lo_fid]\n",
    "y_his = [y[1][c].item() for c in ids_cofs_hi_and_lo_fid]\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot([0, 25], [0, 25], linestyle=\"--\", color=\"k\", linewidth=1)\n",
    "plt.scatter(y_los, y_his, zorder=10)\n",
    "ax = plt.gca()\n",
    "plt.xlim(0, 25)\n",
    "plt.ylim(0, 25)\n",
    "ax.set_aspect(\"equal\", \"box\")\n",
    "plt.xlabel(\"low-fidelity Xe/Kr selectivity\")\n",
    "plt.ylabel(\"high-fidelity Xe/Kr selectivity\")\n",
    "plt.tight_layout()\n",
    "if save_figures:\n",
    "    plt.savefig(\"../figs/lo_vs_hi_fid_selectivity.pdf\", format='pdf')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2a7d1d-3176-4261-94ed-d173e38ee43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get COF ids not in acquired set.\n",
    "test_cof_ids = [cof_id for cof_id in range(nb_COFs) if not (cof_id in hi_fid_cofs)]\n",
    "len(test_cof_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a62f77-432e-42a3-ad25-cfcd82dab491",
   "metadata": {},
   "outputs": [],
   "source": [
    "cof_id_with_max_hi_fid_selectivity in test_cof_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4e707b-8a68-4277-acbb-a806bba28162",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_in_test_cofs_of_opt_cof = np.where([c == cof_id_with_max_hi_fid_selectivity for c in test_cof_ids])[0].item()\n",
    "id_in_test_cofs_of_opt_cof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c7572f-ed5a-45e0-b00a-e6e264549ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train surrogate model for test data, on acquired set up till top COF was found.\n",
    "X_train = build_X_train(ids_acquired[:n_iter_top_cof_found])\n",
    "y_train = build_y_train(ids_acquired[:n_iter_top_cof_found])\n",
    "\n",
    "model = train_surrogate_model(X_train, y_train)\n",
    "\n",
    "# get model predictions on test COFs, for high-fidelity.\n",
    "y_pred, sigma = mu_sigma(model, X[test_cof_ids, :], discrete_fidelities[-1])\n",
    "\n",
    "# plot true vs predicted\n",
    "y_true = [y[1][c].item() for c in test_cof_ids]\n",
    "\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "abserr = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "\n",
    "###\n",
    "#  parity plot\n",
    "###\n",
    "gridspec_kw={'width_ratios': [6, 2], 'height_ratios': [2, 6]} # set ratios\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, gridspec_kw=gridspec_kw, figsize=(8, 8))\n",
    "# fig = plt.figure()\n",
    "ax[0, 1].axis(\"off\")\n",
    "ax[1, 0].plot([0, 20], [0, 20], linestyle=\"--\", color=\"k\", linewidth=1)\n",
    "# ax = plt.gca()\n",
    "ax[1, 0].set_xlim(0, 20)\n",
    "ax[1, 0].set_ylim(0, 20)\n",
    "\n",
    "#ax[1, 0].set_aspect(\"equal\", \"box\")\n",
    "\n",
    "ax[1, 0].text(5, 15, \"R$^2$=%.2f\\nMAE=%.2f\" % (r2, abserr))\n",
    "ax[1, 0].scatter(y_true, y_pred, fc='none', ec=\"k\")\n",
    "ax[1, 0].set_xlabel(\"true\\nhigh-fidelity Xe/Kr selectivity\")\n",
    "ax[1, 0].set_ylabel(\"predicted\\nhigh-fidelity Xe/Kr selectivity\")\n",
    "# plot acquired COF\n",
    "ax[1, 0].scatter(y_true[id_in_test_cofs_of_opt_cof], y_pred[id_in_test_cofs_of_opt_cof], marker=\"x\", color=\"red\")\n",
    "\n",
    "\n",
    "###\n",
    "#  histogram of selectivities\n",
    "###\n",
    "hist_color = sns.color_palette(\"husl\", 8)[7]\n",
    "ax[0, 0].hist(y_true, color=hist_color, alpha=0.5) # \n",
    "ax[0, 0].sharex(ax[1, 0])\n",
    "ax[0, 0].set_ylabel('# COFs')\n",
    "plt.setp(ax[0, 0].get_xticklabels(), visible=False) # remove yticklabels\n",
    "\n",
    "hist_color = sns.color_palette(\"husl\", 8)[7]\n",
    "ax[1, 1].hist(y_pred, color=hist_color, alpha=0.5, orientation=\"horizontal\") # \n",
    "ax[1, 1].sharey(ax[1, 0])\n",
    "ax[1, 1].set_xlabel('# COFs')\n",
    "plt.setp(ax[1, 1].get_yticklabels(), visible=False) # remove yticklabels\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "if save_figures:\n",
    "    plt.savefig(\"../figs/surrogate_parity_with_hist.pdf\", format=\"pdf\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a60c44d-e85a-4705-b475-c59d9dde4995",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_sorted = np.argsort(y_true)[::-1]\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.errorbar(range(len(y_true)), y_pred[ids_sorted], yerr=sigma, linewidth=1, marker=\"o\")\n",
    "plt.errorbar(0, y_pred[id_in_test_cofs_of_opt_cof], yerr=sigma[id_in_test_cofs_of_opt_cof], linewidth=1, marker=\"o\", color=\"red\")\n",
    "plt.xlabel(\"rank according to true high-fidelity Xe/Kr selectivity\")\n",
    "plt.ylabel(\"predicted\\nhigh-fidelity\\nXe/Kr selectivity\")\n",
    "plt.xlim(-1, len(test_cof_ids) +1)\n",
    "plt.tight_layout()\n",
    "if save_figures:\n",
    "    plt.savefig(\"../figs/surrogate_predictions.pdf\", format=\"pdf\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207568dd-c325-49b6-88ad-96c862c42142",
   "metadata": {},
   "outputs": [],
   "source": [
    "cof_id_with_max_hi_fid_selectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47061e4c-bce9-49bd-99d1-d9bbf1a64040",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where([cof_id == cof_id_with_max_hi_fid_selectivity for cof_id in test_cof_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b443dd-a64c-4733-b888-0eab86a44719",
   "metadata": {},
   "source": [
    "# Run MFBO under different initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d66fd66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "#  run search\n",
    "###\n",
    "for j, initializing_COFs in enumerate(init_cof_ids): \n",
    "\n",
    "    # each time, randomly shuffle features (with no info in them about the COFs)\n",
    "    if ablation_study_flag:\n",
    "        for j in range(X.size()[1]):\n",
    "            row_ids = torch.randperm(X.size()[0])\n",
    "            X[:, j] = X[row_ids, j]\n",
    "\n",
    "            \n",
    "    # check the length of each initializing set\n",
    "    assert len(initializing_COFs) == nb_COFs_initialization\n",
    "    print(\"run #: {}\".format(j))\n",
    "    \n",
    "    # start timer for BO run\n",
    "    start_time = time.time()\n",
    "    ###\n",
    "    #  run BO search\n",
    "    ###\n",
    "    acquired_set = run_Bayesian_optimization(nb_iterations, initializing_COFs)\n",
    "    \n",
    "    ###\n",
    "    #  post-run analysis\n",
    "    ###\n",
    "    elapsed_time  = time.time() - start_time\n",
    "    print(\"elapsed_time:\\t\", elapsed_time / 60, \" min.\")\n",
    "    y_acquired    = build_y_train(acquired_set)\n",
    "    y_maxes_acq   = get_y_maxes_acquired(acquired_set.detach().numpy())\n",
    "    fid_fraction  = calc_fidelity_fraction(acquired_set.detach().numpy(), discrete_fidelities[1])\n",
    "    cost_acquired = build_cost(acquired_set)\n",
    "    acc_cost      = accumulated_cost(cost_acquired.flatten().detach().numpy())\n",
    "    \n",
    "    ###\n",
    "    # look at unique COFs acquired\n",
    "    ###\n",
    "    # cof_ids_acquired = torch.tensor([acq[1] for acq in acquired_set])\n",
    "    n_unique_cofs_acquired = len(np.unique([acq[1] for acq in acquired_set]))\n",
    "    print(\"total number of unique COFs acquired\", n_unique_cofs_acquired)\n",
    "\n",
    "    ###\n",
    "    #  Iterations until top COF and accumulated \n",
    "    ###\n",
    "    cof_id_with_max_selectivity = np.argmax(y[1])\n",
    "    BO_iter_top_cof_acquired = float(\"inf\") # dummy \n",
    "    for i, (f_id, cof_id) in enumerate(acquired_set):\n",
    "        if cof_id.to(dtype=int) == cof_id_with_max_selectivity and get_f_ids(f_id) == 1:\n",
    "            BO_iter_top_cof_acquired = i\n",
    "            print(\"woo, top COF acquired!\")\n",
    "            print(\"iteration we acquire top COF = \", BO_iter_top_cof_acquired) \n",
    "            break\n",
    "        elif i == len(acquired_set)-1:\n",
    "            print(\"oh no, top COF not acquired!\")\n",
    "\n",
    "    # print accumulated cost to find the optimal COF\n",
    "    top_cof_acc_cost = sum(build_cost(acquired_set)[:BO_iter_top_cof_acquired])\n",
    "    print(\"accumulated cost up to observation of top COF = \", top_cof_acc_cost.item(), \" [min]\")\n",
    "    \n",
    "    ###\n",
    "    #  store results\n",
    "    ###\n",
    "    mfbo_res = dict({'acquired_set': acquired_set.detach().numpy(),\n",
    "                     'y_acquired': y_acquired.detach().numpy(),\n",
    "                     'y_max_acquired': y_maxes_acq,\n",
    "                     'fidelity_fraction': fid_fraction,\n",
    "                     'cost_acquired': cost_acquired.flatten().detach().numpy(),\n",
    "                     'accumulated_cost': acc_cost / 60,\n",
    "                     'nb_COFs_initialization': nb_COFs_initialization,\n",
    "                     'BO_iter_top_cof_acquired': BO_iter_top_cof_acquired,\n",
    "                     'elapsed_time (min)':  elapsed_time / 60\n",
    "                    })\n",
    "    pickle_filename = '../search_results/mfbo_results/mfbo_results_run_{}'.format(j)\n",
    "    if ablation_study_flag:\n",
    "        pickle_filename += \"_ablation\"\n",
    "    pickle_filename += \".pkl\"\n",
    "    with open(pickle_filename, 'wb') as file:\n",
    "        pickle.dump(mfbo_res, file)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "873b9eed737f4a6d896d154d73024f53",
   "lastKernelId": "464ee535-3ea4-4199-a625-569df1ee6433"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "23d6f7179e0bc929ef338139025b84bf12b8362d4f462c6c3a682d200e09aab4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
