{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d087dd2",
   "metadata": {},
   "source": [
    "## System Description\n",
    "1. We have a set of COFs from a database. Each COF is characterized by a feature vector $$x_{COF} \\in X \\subset R^d$$ were d=14.\n",
    "\n",
    "\n",
    "2. We have **two different types** of simulations to calculate **the same material property $S_{Xe/Kr}$**. Therefore, we have a Single-Task/Objective \n",
    "$$argmax_{x_{COF} \\in X}[S_{Xe/Kr}(x_{COF})]$$\n",
    "\n",
    "3. Multi-Fidelity problem. \n",
    "    1. low-fidelity  => Henry coefficient calculation - MC integration: $S_{Xe/Kr} = \\frac{H_{Xe}}{H_{Kr}}$\n",
    "    2. high-fidelity => GCMC mixture simulation - 80:20 (Kr:Xe) at 298 K and 1.0 bar: $S_{Xe/Kr} = \\frac{n_{Xe} / n_{Kr}}{y_{Xe}/y_{Kr}}$\n",
    "\n",
    "\n",
    "3. We will initialize the system with a few COFs at **both** fidelities in order to initialize the Covariance Matrix.\n",
    "    1. The fist COF will be the one closest to the center of the normalized feature space\n",
    "    2. The rest will be chosen to maximize diversity of the training set\n",
    "\n",
    "\n",
    "4. Each surrogate model will **only train on data acquired at its level of fidelity** (Heterotopic data). $$X_{lf} \\neq X_{hf} \\subset X$$\n",
    "    1. We  use the augmented-EI (aEI) acquisition function from [here](https://link.springer.com/content/pdf/10.1007/s00158-005-0587-0.pdf)\n",
    "    2. Botorch GP surrogate model: [SingleTaskMultiFidelityGP](https://botorch.org/api/models.html#module-botorch.models.gp_regression_fidelity)\n",
    "    3. Needed to use [this](https://botorch.org/api/optim.html#module-botorch.optim.fit) optimizer to correct matrix jitter\n",
    "    4. Helpful [tutorial](https://botorch.org/tutorials/discrete_multi_fidelity_bo) for a similar BoTorch Model used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "669c708a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simoncor/anaconda3/lib/python3.9/site-packages/torch/cuda/__init__.py:83: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "from botorch.models import SingleTaskMultiFidelityGP\n",
    "from botorch.models.transforms.outcome import Standardize\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch import fit_gpytorch_model\n",
    "from botorch.optim.fit import fit_gpytorch_torch # fix Cholecky jitter error\n",
    "from scipy.stats import norm\n",
    "from sklearn.decomposition import PCA\n",
    "import math \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import h5py # for .jld2 files\n",
    "import os\n",
    "\n",
    "# config plot settings\n",
    "plt.rcParams[\"font.size\"] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1c603cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw data - \n",
      "\tX: torch.Size([608, 14])\n",
      "\tfidelity: 0\n",
      "\t\ty: torch.Size([608])\n",
      "\t\tcost:  (608,)\n",
      "\tfidelity: 1\n",
      "\t\ty: torch.Size([608])\n",
      "\t\tcost:  (608,)\n",
      "\n",
      "Ensure features are normalized - \n",
      "max:\n",
      " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       dtype=torch.float64)\n",
      "min:\n",
      " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "#  Load Data\n",
    "###\n",
    "file = h5py.File(\"targets_and_normalized_features.jld2\", \"r\")\n",
    "# feature matrix\n",
    "X = torch.from_numpy(np.transpose(file[\"X\"][:]))\n",
    "# simulation data\n",
    "y = [torch.from_numpy(np.transpose(file[\"henry_y\"][:])), \n",
    "     torch.from_numpy(np.transpose(file[\"gcmc_y\"][:]))]\n",
    "# associated simulation costs\n",
    "cost = [np.transpose(file[\"henry_total_elapsed_time\"][:]), \n",
    "        np.transpose(file[\"gcmc_elapsed_time\"][:])]\n",
    "\n",
    "# total number of COFs in data set\n",
    "nb_COFs = X.shape[0] \n",
    "\n",
    "print(\"raw data - \\n\\tX:\", X.shape)\n",
    "for f in range(2):\n",
    "    print(\"\\tfidelity:\", f)\n",
    "    print(\"\\t\\ty:\", y[f].shape)\n",
    "    print(\"\\t\\tcost: \", cost[f].shape)\n",
    "    \n",
    "print(\"\\nEnsure features are normalized - \")\n",
    "print(\"max:\\n\", torch.max(X, 0).values)\n",
    "print(\"min:\\n\", torch.min(X, 0).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfcd12ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total high-fidelity cost: 139887.66223703226 [min]\n",
      "total low-fidelity cost:  10076.305239888028 [min]\n",
      "\n",
      "average high-fidelity cost: 230.0783918372241 [min]\n",
      "average low-fidelity cost:  16.57287046034216 [min]\n",
      "average cost ratio:\t    13.444745568580501\n"
     ]
    }
   ],
   "source": [
    "print(\"total high-fidelity cost:\", sum(cost[1]).item(), \"[min]\")\n",
    "print(\"total low-fidelity cost: \", sum(cost[0]).item(), \"[min]\\n\")\n",
    "\n",
    "print(\"average high-fidelity cost:\", np.mean(cost[1]), \"[min]\")\n",
    "print(\"average low-fidelity cost: \", np.mean(cost[0]), \"[min]\")\n",
    "print(\"average cost ratio:\\t   \", np.mean(cost[1] / cost[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f9ab59",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "#### Construct Initial Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fbc1845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find COF closest to the center of feature space\n",
    "def get_initializing_COF(X):\n",
    "    # center of feature space\n",
    "    feature_center = np.ones(X.shape[1]) * 0.5\n",
    "    # max possible distance between normalized features\n",
    "    return np.argmin(np.linalg.norm(X - feature_center, axis=1))\n",
    "\n",
    "# yields np.array([25, 494, 523])\n",
    "def diverse_set(X, train_size):\n",
    "    # initialize with one random point; pick others in a max diverse fashion\n",
    "    ids_train = [get_initializing_COF(X)]\n",
    "    # select remaining training points\n",
    "    for j in range(train_size - 1):\n",
    "        # for each point in data set, compute its min dist to training set\n",
    "        dist_to_train_set = np.linalg.norm(X - X[ids_train, None, :], axis=2)\n",
    "        assert np.shape(dist_to_train_set) == (len(ids_train), nb_COFs)\n",
    "        min_dist_to_a_training_pt = np.min(dist_to_train_set, axis=0)\n",
    "        assert np.size(min_dist_to_a_training_pt) == nb_COFs\n",
    "        \n",
    "        # acquire point with max(min distance to train set) i.e. Furthest from train set\n",
    "        ids_train.append(np.argmax(min_dist_to_a_training_pt))\n",
    "    assert np.size(np.unique(ids_train)) == train_size # must be unique\n",
    "    return np.array(ids_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f187575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_acquired_set(X, y, nb_COFs_initialization, discrete_fidelities):\n",
    "    cof_ids = diverse_set(X, nb_COFs_initialization) # np.array(ids_train)\n",
    "    return torch.tensor([[f_id, cof_id] for cof_id in cof_ids for f_id in discrete_fidelities])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "300bcf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct feature matrix of acquired points\n",
    "def build_X_train(acquired_set):\n",
    "    cof_ids = [a[1] for a in acquired_set]\n",
    "    f_ids = torch.tensor([a[0] for a in acquired_set])\n",
    "    return torch.cat((X[cof_ids, :], f_ids.unsqueeze(dim=-1)), dim=1)\n",
    "\n",
    "# construct output vector for acquired points\n",
    "def build_y_train(acquired_set):\n",
    "    return torch.tensor([y[f_id][cof_id] for f_id, cof_id in acquired_set]).unsqueeze(-1)\n",
    "\n",
    "# construct vector to track accumulated cost of acquired points\n",
    "def build_cost(acquired_set):\n",
    "    return torch.tensor([cost[f_id][cof_id] for f_id, cof_id in acquired_set]).unsqueeze(-1)\n",
    "\n",
    "# construct vector to track accumulated cost of acquired points\n",
    "def build_cost_fidelity(acquired_set, fidelity):\n",
    "    return torch.tensor([cost[f_id][cof_id] for f_id, cof_id in acquired_set if f_id == fidelity]).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec40fbc",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f1f9678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_initializing_functions(X, y):\n",
    "    ###\n",
    "    #  Construct training sets\n",
    "    ###\n",
    "    # list of (cof_id, fid_id)'s\n",
    "    acquired_set = [[1, 10], [0, 3], [0, 4]]\n",
    "    \n",
    "    # Training Sets\n",
    "    X_train = build_X_train(acquired_set)\n",
    "    y_train = build_y_train(acquired_set)\n",
    "    \n",
    "    ###\n",
    "    #  Test that the constructor functions are working properly\n",
    "    ###\n",
    "    assert np.allclose(X[10, :], X_train[0, :14])\n",
    "    assert X_train[0, 14] == 1\n",
    "    assert X_train[1, 14] == 0\n",
    "    assert y_train[0] == y[1][10] # y[fid_id][cof_id]\n",
    "    assert y_train[2] == y[0][4]\n",
    "    return\n",
    "\n",
    "test_initializing_functions(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d2ef9b",
   "metadata": {},
   "source": [
    "#### Surrogate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b21c9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_surrogate_model(X_train, y_train):\n",
    "    model = SingleTaskMultiFidelityGP(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        outcome_transform=Standardize(m=1), # m is the output dimension\n",
    "        data_fidelity=X_train.shape[1] - 1\n",
    "    )   \n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    fit_gpytorch_model(mll, optimizer=fit_gpytorch_torch)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a87aeb",
   "metadata": {},
   "source": [
    "#### Acquisition Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6672eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate posterior mean and variance for a given fidelity\n",
    "def mu_sigma(model, X, fidelity):\n",
    "    f = torch.tensor((), dtype=torch.float64).new_ones((nb_COFs, 1)) * fidelity\n",
    "    X_f = torch.cat((X, f), dim=1) # last col is associated fidelity\n",
    "    f_posterior = model.posterior(X_f)\n",
    "    return f_posterior.mean.squeeze().detach().numpy(), np.sqrt(f_posterior.variance.squeeze().detach().numpy())\n",
    "\n",
    "# get the current best y-value of desired_fidelity in the acquired set\n",
    "def get_y_max(acquired_set, desired_fidelity):\n",
    "    return np.max([y[f_id][cof_id] for f_id, cof_id in acquired_set if f_id == desired_fidelity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ccf3d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# efficient multi-fidelity correlation function\n",
    "# corr(y at given fidelity, y at high-fidelity)\n",
    "# (see notes)\n",
    "###\n",
    "def mfbo_correlation_function(model, X, fidelity):\n",
    "    # given fidelity\n",
    "    f   = torch.tensor((), dtype=torch.float64).new_ones((nb_COFs, 1)) * fidelity\n",
    "    X_f = torch.cat((X, f), dim=1) # last col is associated fidelity\n",
    "    \n",
    "    #  high-fidelity\n",
    "    hf   = torch.tensor((), dtype=torch.float64).new_ones((nb_COFs, 1)) \n",
    "    X_hf = torch.cat((X, hf), dim=1) # last col is associated fidelity\n",
    "\n",
    "    # combine into a single tensor\n",
    "    X_all_fid = torch.cat((X_f, X_hf), dim=0)\n",
    "    \n",
    "    # get variance for each fidelity\n",
    "    var_f = torch.flatten(model.posterior(X_f).variance)\n",
    "    var_hf = torch.flatten(model.posterior(X_hf).variance) # variance\n",
    "    \n",
    "    # posterior covariance \n",
    "    cov = torch.diag(model(X_all_fid).covariance_matrix[:X_f.size()[0], X_f.size()[0]:])\n",
    "    \n",
    "    corr = cov / (torch.sqrt(var_f) * torch.sqrt(var_hf))\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25b278ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#  Cost ratio\n",
    "###\n",
    "def estimate_cost_ratio(fidelity, acquired_set):\n",
    "    avg_cost_f  = torch.mean(build_cost_fidelity(acquired_set, fidelity))\n",
    "    avg_cost_hf = torch.mean(build_cost_fidelity(acquired_set, 1))\n",
    "    cr = avg_cost_hf / avg_cost_f\n",
    "    return cr.item()\n",
    "\n",
    "###\n",
    "#  Expected Imrovement function, only uses hf\n",
    "###\n",
    "def EI_hf(model, X, acquired_set):\n",
    "    hf_mu, hf_sigma = mu_sigma(model, X, 1)\n",
    "    y_max = get_y_max(acquired_set, 1)\n",
    "    \n",
    "    z = (hf_mu - y_max) / hf_sigma\n",
    "    explore_term = hf_sigma * norm.pdf(z) \n",
    "    exploit_term = (hf_mu - y_max) * norm.cdf(z) \n",
    "    ei = explore_term + exploit_term\n",
    "    return np.maximum(ei, np.zeros(nb_COFs))\n",
    "\n",
    "###\n",
    "#  Acquisition function\n",
    "###\n",
    "def acquisition_scores(model, X, fidelity, acquired_set):\n",
    "    # expected improvement for high-fidelity\n",
    "    ei = EI_hf(model, X, acquired_set) \n",
    "    \n",
    "    # augmenting functions\n",
    "    corr_f1_f0 = mfbo_correlation_function(model, X, fidelity)\n",
    "    \n",
    "    cr = estimate_cost_ratio(fidelity, acquired_set)\n",
    "\n",
    "    scores = torch.from_numpy(ei) * corr_f1_f0 * cr\n",
    "    return scores.detach().numpy()\n",
    "\n",
    "def in_acquired_set(f_id, cof_id, acquired_set):\n",
    "    for this_f_id, this_cof_id in acquired_set:\n",
    "        if this_cof_id == cof_id and this_f_id == f_id:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2130854c",
   "metadata": {},
   "source": [
    "# Run MFBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa07e691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization - \n",
      "\n",
      "\tid acquired =  [0, 1, 0, 1, 0, 1]\n",
      "\tfidelity acquired =  [25, 25, 494, 494, 523, 523]\n",
      "\tcosts acquired =  tensor([[ 33.2507],\n",
      "        [399.7577],\n",
      "        [ 33.2389],\n",
      "        [171.9985],\n",
      "        [  6.1207],\n",
      "        [280.4524]], dtype=torch.float64)  [min]\n",
      "\n",
      "\tTraining data:\n",
      "\n",
      "\t\t X train shape =  torch.Size([6, 15])\n",
      "\t\t y train shape =  torch.Size([6, 1])\n",
      "\t\t training feature vector = \n",
      " tensor([[0.1500, 0.4533, 0.1088, 0.5523, 0.4387, 0.1463, 0.3480, 0.2643, 0.0000,\n",
      "         0.1769, 0.2237, 0.0000, 0.0000, 0.3471, 0.0000],\n",
      "        [0.1500, 0.4533, 0.1088, 0.5523, 0.4387, 0.1463, 0.3480, 0.2643, 0.0000,\n",
      "         0.1769, 0.2237, 0.0000, 0.0000, 0.3471, 1.0000],\n",
      "        [0.8347, 0.9979, 0.9053, 0.0040, 0.0161, 0.0623, 0.0000, 0.0095, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.8347, 0.9979, 0.9053, 0.0040, 0.0161, 0.0623, 0.0000, 0.0095, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
      "        [0.0509, 0.2570, 0.4148, 0.6881, 0.4814, 0.4971, 0.8866, 0.0000, 0.0000,\n",
      "         0.0000, 0.3800, 1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0509, 0.2570, 0.4148, 0.6881, 0.4814, 0.4971, 0.8866, 0.0000, 0.0000,\n",
      "         0.0000, 0.3800, 1.0000, 0.0000, 0.0000, 1.0000]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "#  construct initial inputs\n",
    "###\n",
    "discrete_fidelities = [0, 1] # set of discrete fidelities to select from\n",
    "nb_COFs_initialization = 3   # at each fidelity, number of COFs to initialize with\n",
    "nb_iterations = 100          # BO budget, includes initializing COFs\n",
    "\n",
    "acquired_set = initialize_acquired_set(X, y, nb_COFs_initialization, discrete_fidelities)\n",
    "\n",
    "X_train = build_X_train(acquired_set)\n",
    "y_train = build_y_train(acquired_set)\n",
    "\n",
    "print(\"Initialization - \\n\")\n",
    "print(\"\\tid acquired = \", [acq_[0].item() for acq_ in acquired_set])\n",
    "print(\"\\tfidelity acquired = \", [acq_[1].item() for acq_ in acquired_set])\n",
    "print(\"\\tcosts acquired = \", build_cost(acquired_set), \" [min]\")\n",
    "\n",
    "print(\"\\n\\tTraining data:\\n\")\n",
    "print(\"\\t\\t X train shape = \", X_train.shape)\n",
    "print(\"\\t\\t y train shape = \", y_train.shape)\n",
    "print(\"\\t\\t training feature vector = \\n\", X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae82c1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BO iteration:  6\n",
      "Iter 10/100: 8.649342013940094\n",
      "Iter 20/100: 6.261137550553084\n",
      "Iter 30/100: 4.990258879375678\n",
      "Iter 40/100: 4.189199617403134\n",
      "Iter 50/100: 3.970566587573529\n",
      "Iter 60/100: 3.861769073961766\n",
      "Iter 70/100: 3.8169358792521293\n",
      "Iter 80/100: 3.7957507575624336\n",
      "Iter 90/100: 3.786184368275768\n",
      "Iter 100/100: 3.779489479113306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simoncor/anaconda3/lib/python3.9/site-packages/botorch/fit.py:148: UserWarning: torch.triangular_solve is deprecated in favor of torch.linalg.solve_triangularand will be removed in a future PyTorch release.\n",
      "torch.linalg.solve_triangular has its arguments reversed and does not return a copy of one of the inputs.\n",
      "X = torch.triangular_solve(B, A).solution\n",
      "should be replaced with\n",
      "X = torch.linalg.solve_triangular(A, B). (Triggered internally at  ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2189.)\n",
      "  warnings.warn(w.message, w.category)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tacquired COF  521  at fidelity,  0\n",
      "\t\ty =  15.30972918212556\n",
      "\t\tcost =  6.206935115655263\n",
      "BO iteration:  7\n",
      "Iter 10/100: 7.614303650099994\n",
      "Iter 20/100: 5.5481737176964065\n",
      "Iter 30/100: 4.4193375753205375\n",
      "Iter 40/100: 3.6769469794672327\n",
      "Iter 50/100: 3.513558616402799\n",
      "Iter 60/100: 3.4289432847700096\n",
      "Iter 70/100: 3.3896726465679157\n",
      "Iter 80/100: 3.3695011054234394\n",
      "Iter 90/100: 3.3575767043886984\n",
      "Iter 100/100: 3.347487472544626\n",
      "\tacquired COF  522  at fidelity,  0\n",
      "\t\ty =  15.951650673989429\n",
      "\t\tcost =  6.0286850492159525\n",
      "BO iteration:  8\n",
      "Iter 10/100: 6.850133783119505\n",
      "Iter 20/100: 5.027511893864176\n",
      "Iter 30/100: 4.017520396009643\n",
      "Iter 40/100: 3.334977681387051\n",
      "Iter 50/100: 3.207249960866922\n",
      "Iter 60/100: 3.1371892739627727\n",
      "Iter 70/100: 3.1022503356921383\n",
      "Iter 80/100: 3.0824577383154748\n",
      "Iter 90/100: 3.0693220621859405\n",
      "Iter 100/100: 3.0569681276770346\n",
      "\tacquired COF  583  at fidelity,  0\n",
      "\t\ty =  2.848405682526983\n",
      "\t\tcost =  24.38421953121821\n",
      "BO iteration:  9\n",
      "Iter 10/100: 6.2718023493026855\n",
      "Iter 20/100: 4.643357801296398\n",
      "Iter 30/100: 3.740426745638075\n",
      "Iter 40/100: 3.141060587078603\n",
      "Iter 50/100: 3.0193826350508552\n",
      "Iter 60/100: 2.9564958502217484\n",
      "Iter 70/100: 2.925718086578548\n",
      "Iter 80/100: 2.9084775774754164\n",
      "Iter 90/100: 2.8971051963217445\n",
      "Iter 100/100: 2.886593765264876\n",
      "\tacquired COF  524  at fidelity,  0\n",
      "\t\ty =  6.472940599293585\n",
      "\t\tcost =  4.269400648276012\n",
      "BO iteration:  10\n",
      "Iter 10/100: 5.797035457331449\n",
      "Iter 20/100: 4.3222469419462115\n",
      "Iter 30/100: 3.4959332215807364\n",
      "Iter 40/100: 2.940039168215459\n",
      "Iter 50/100: 2.835743445418519\n",
      "Iter 60/100: 2.7809874904928242\n",
      "Iter 70/100: 2.752928918406733\n",
      "Iter 80/100: 2.7362107236593474\n",
      "Iter 90/100: 2.7243197830185286\n",
      "Iter 100/100: 2.712888331334978\n",
      "\tacquired COF  0  at fidelity,  0\n",
      "\t\ty =  1.5805050493821187\n",
      "\t\tcost =  3.4525069634119667\n",
      "BO iteration:  11\n",
      "Iter 10/100: 5.410963474687919\n",
      "Iter 20/100: 4.06274673674083\n",
      "Iter 30/100: 3.302890678895645\n",
      "Iter 40/100: 2.785478811229791\n",
      "Iter 50/100: 2.6950047394136174\n",
      "Iter 60/100: 2.646480505438719\n",
      "Iter 70/100: 2.6205218974928712\n",
      "Iter 80/100: 2.6042852880133784\n",
      "Iter 90/100: 2.5921223027418616\n",
      "Iter 100/100: 2.5801386983414685\n",
      "\tacquired COF  223  at fidelity,  0\n",
      "\t\ty =  4.261607264546673\n",
      "\t\tcost =  9.34896615346273\n",
      "BO iteration:  12\n",
      "Iter 10/100: 5.086933525786033\n",
      "Iter 20/100: 3.843600431682356\n",
      "Iter 30/100: 3.1370631108644376\n",
      "Iter 40/100: 2.6518602722625304\n",
      "Iter 50/100: 2.5721323584181914\n",
      "Iter 60/100: 2.5291123717071327\n",
      "Iter 70/100: 2.504536467156498\n",
      "Iter 80/100: 2.4884889587365\n",
      "Iter 90/100: 2.476085560457181\n",
      "Iter 100/100: 2.4636884880333727\n",
      "\tacquired COF  9  at fidelity,  0\n",
      "\t\ty =  4.5299255458988075\n",
      "\t\tcost =  28.559914251168568\n",
      "BO iteration:  13\n",
      "Iter 10/100: 4.812050790221514\n",
      "Iter 20/100: 3.657037679334282\n",
      "Iter 30/100: 2.994754708400862\n",
      "Iter 40/100: 2.537096906510757\n",
      "Iter 50/100: 2.4653804145099474\n",
      "Iter 60/100: 2.426060597368748\n",
      "Iter 70/100: 2.4029613312537967\n",
      "Iter 80/100: 2.3871391318429356\n",
      "Iter 90/100: 2.3742498101482385\n",
      "Iter 100/100: 2.361101097984507\n",
      "\tacquired COF  73  at fidelity,  0\n",
      "\t\ty =  4.159682969326826\n",
      "\t\tcost =  3.758783952395121\n",
      "BO iteration:  14\n",
      "Iter 10/100: 4.57625734723482\n",
      "Iter 20/100: 3.496377181225939\n",
      "Iter 30/100: 2.8731446404679724\n",
      "Iter 40/100: 2.437083634011235\n",
      "Iter 50/100: 2.3732205991159803\n",
      "Iter 60/100: 2.3373040404022976\n",
      "Iter 70/100: 2.315074190201187\n",
      "Iter 80/100: 2.299168615051625\n",
      "Iter 90/100: 2.2856655874200533\n",
      "Iter 100/100: 2.271617504498877\n",
      "\tacquired COF  338  at fidelity,  0\n",
      "\t\ty =  1.1153962862564593\n",
      "\t\tcost =  34.08373984893163\n",
      "BO iteration:  15\n",
      "Iter 10/100: 4.371211452564325\n",
      "Iter 20/100: 3.355643587206589\n",
      "Iter 30/100: 2.766002260312492\n",
      "Iter 40/100: 2.3435662950776903\n",
      "Iter 50/100: 2.290691666661148\n",
      "Iter 60/100: 2.257764189324116\n",
      "Iter 70/100: 2.2357292997595963\n",
      "Iter 80/100: 2.219439795013405\n",
      "Iter 90/100: 2.2047956463305147\n",
      "Iter 100/100: 2.1890495462846604\n",
      "\tacquired COF  520  at fidelity,  0\n",
      "\t\ty =  6.140702426978196\n",
      "\t\tcost =  11.222975416978201\n",
      "BO iteration:  16\n",
      "Iter 10/100: 4.193318867629736\n",
      "Iter 20/100: 3.2342484852678033\n",
      "Iter 30/100: 2.6768760090664783\n",
      "Iter 40/100: 2.2757916563839835\n",
      "Iter 50/100: 2.227567434247415\n",
      "Iter 60/100: 2.1946289678269055\n",
      "Iter 70/100: 2.1738652975204595\n",
      "Iter 80/100: 2.1582856810827025\n",
      "Iter 90/100: 2.1438461032396026\n",
      "Iter 100/100: 2.1285196288144768\n",
      "\tacquired COF  521  at fidelity,  1\n",
      "\t\ty =  15.766064256252303\n",
      "\t\tcost =  602.2006956656774\n",
      "BO iteration:  17\n",
      "Iter 10/100: 4.022939400143419\n",
      "Iter 20/100: 3.108918026627675\n",
      "Iter 30/100: 2.56153856893429\n",
      "Iter 40/100: 2.2010808630384293\n",
      "Iter 50/100: 2.0977188824195245\n",
      "Iter 60/100: 2.0332277098575795\n",
      "Iter 70/100: 2.010542058556338\n",
      "Iter 80/100: 2.0019416429841375\n",
      "Iter 90/100: 1.9996302957913528\n",
      "Iter 100/100: 1.9988339572267126\n",
      "\tacquired COF  98  at fidelity,  0\n",
      "\t\ty =  0.07902535518803018\n",
      "\t\tcost =  1.1220028042793273\n",
      "BO iteration:  18\n",
      "Iter 10/100: 3.882987283285372\n",
      "Iter 20/100: 3.0123359046198317\n",
      "Iter 30/100: 2.487698350368662\n",
      "Iter 40/100: 2.1357869479819374\n",
      "Iter 50/100: 2.0409816914705554\n",
      "Iter 60/100: 1.9789466745537965\n",
      "Iter 70/100: 1.9559972996331487\n",
      "Iter 80/100: 1.9472074100436099\n",
      "Iter 90/100: 1.9448952131474355\n",
      "Iter 100/100: 1.9442446883097533\n",
      "\tacquired COF  224  at fidelity,  0\n",
      "\t\ty =  3.6801115645243847\n",
      "\t\tcost =  8.110053702195486\n",
      "BO iteration:  19\n",
      "Iter 10/100: 3.7551051894374834\n",
      "Iter 20/100: 2.92352857232499\n",
      "Iter 30/100: 2.4179700088372456\n",
      "Iter 40/100: 2.0749937607901074\n",
      "Iter 50/100: 1.9862819356275954\n",
      "Iter 60/100: 1.9262916896644107\n",
      "Iter 70/100: 1.904122222789806\n",
      "Iter 80/100: 1.8956302067471127\n",
      "Iter 90/100: 1.8935676965255552\n",
      "Iter 100/100: 1.8929365190444016\n",
      "\tacquired COF  463  at fidelity,  0\n",
      "\t\ty =  5.863058699494186\n",
      "\t\tcost =  22.66465273300807\n",
      "BO iteration:  20\n",
      "Iter 10/100: 3.6426702171568364\n",
      "Iter 20/100: 2.8464873925543066\n",
      "Iter 30/100: 2.3608131762747493\n",
      "Iter 40/100: 2.0306054249634493\n",
      "Iter 50/100: 1.9485752482603473\n",
      "Iter 60/100: 1.8917955766319436\n",
      "Iter 70/100: 1.8686144352467502\n",
      "Iter 80/100: 1.8596995649769439\n",
      "Iter 90/100: 1.8575293712497452\n",
      "Iter 100/100: 1.8570261352608444\n",
      "\tacquired COF  155  at fidelity,  0\n",
      "\t\ty =  8.124910921699056\n",
      "\t\tcost =  4.178414964675904\n",
      "BO iteration:  21\n",
      "Iter 10/100: 3.5432402522402215\n",
      "Iter 20/100: 2.780181863842272\n",
      "Iter 30/100: 2.3164283310446567\n",
      "Iter 40/100: 2.0038740078797654\n",
      "Iter 50/100: 1.9279020514576288\n",
      "Iter 60/100: 1.8710157181123876\n",
      "Iter 70/100: 1.8492192407973147\n",
      "Iter 80/100: 1.8406236382648449\n",
      "Iter 90/100: 1.8377480242417565\n",
      "Iter 100/100: 1.8369214653579686\n",
      "\tacquired COF  84  at fidelity,  0\n",
      "\t\ty =  3.8805251683537945\n",
      "\t\tcost =  14.227486248811086\n",
      "BO iteration:  22\n",
      "Iter 10/100: 3.447292196192218\n",
      "Iter 20/100: 2.713600906764736\n",
      "Iter 30/100: 2.263376747003274\n",
      "Iter 40/100: 1.9590682194657647\n",
      "Iter 50/100: 1.8868870829069075\n",
      "Iter 60/100: 1.8325593864202885\n",
      "Iter 70/100: 1.8113787846152098\n",
      "Iter 80/100: 1.803163505549936\n",
      "Iter 90/100: 1.80038605944402\n",
      "Iter 100/100: 1.799581834511554\n",
      "\tacquired COF  262  at fidelity,  0\n",
      "\t\ty =  0.6837883369606389\n",
      "\t\tcost =  7.507218949000041\n",
      "BO iteration:  23\n",
      "Iter 10/100: 3.3610489806026407\n",
      "Iter 20/100: 2.6543220290959355\n",
      "Iter 30/100: 2.2185730785734505\n",
      "Iter 40/100: 1.92904029847554\n",
      "Iter 50/100: 1.8595872103430224\n",
      "Iter 60/100: 1.8082768857420635\n",
      "Iter 70/100: 1.788412230689716\n",
      "Iter 80/100: 1.7807262375520856\n",
      "Iter 90/100: 1.7780298469766134\n",
      "Iter 100/100: 1.7772260090364658\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "#  Run Search\n",
    "###\n",
    "for i in range(nb_COFs_initialization * len(discrete_fidelities), nb_iterations): \n",
    "    print(\"BO iteration: \", i)\n",
    "    ###\n",
    "    #  Train Model\n",
    "    ###\n",
    "    model = train_surrogate_model(X_train, y_train)\n",
    "\n",
    "    ###\n",
    "    # Acquire new (COF, fidelity) not yet acquired.\n",
    "    ###\n",
    "    # entry (fid_id, cof_id) is the acquisition value for fidelity f_id and cof cof_id\n",
    "    the_acquisition_scores = np.array([acquisition_scores(model, X, f_id, acquired_set) for f_id in discrete_fidelities])\n",
    "    # overwrite acquired COFs/fidelities with negative infinity to not choose these.\n",
    "    for f_id, cof_id in acquired_set:\n",
    "        the_acquisition_scores[f_id, cof_id] = - np.inf\n",
    "    # select COF/fidelity with highest aquisition score.\n",
    "    f_id, cof_id = np.unravel_index(np.argmax(the_acquisition_scores), np.shape(the_acquisition_scores))\n",
    "    assert not in_acquired_set(f_id, cof_id, acquired_set)\n",
    "    # Update acquired_set\n",
    "    acq = torch.tensor([[f_id, cof_id]], dtype=int)\n",
    "    acquired_set = torch.cat((acquired_set, acq))\n",
    "    \n",
    "    ###\n",
    "    #\n",
    "    ###\n",
    "    print(\"\\tacquired COF \", cof_id, \" at fidelity, \", f_id)\n",
    "    print(\"\\t\\ty = \", y[f_id][cof_id].item())\n",
    "    print(\"\\t\\tcost = \", cost[f_id][cof_id])\n",
    "            \n",
    "    # Update training sets\n",
    "    X_train = build_X_train(acquired_set)\n",
    "    y_train = build_y_train(acquired_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b72adb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# look at unique COFs acquired\n",
    "###\n",
    "cof_ids_acquired = torch.tensor([acq[1] for acq in acquired_set])\n",
    "n_unique_cofs_acquired = len(np.unique(cof_ids_acquired))\n",
    "print(\"total number of unique COFs acquired\", n_unique_cofs_acquired)\n",
    "\n",
    "###\n",
    "#  Iterations until top COF and accumulated \n",
    "###\n",
    "if np.argmax(y[1]) in cof_ids_acquired:\n",
    "    print(\"woo, top COF acquired!\")\n",
    "else:\n",
    "    print(\"oh no, top COF not acquired!\")\n",
    "    \n",
    "BO_iter_top_cof_acquired = np.argmax(cof_ids_acquired == np.argmax(y[1]))\n",
    "top_cof_acc_cost = sum(build_cost(acquired_set)[:BO_iter_top_cof_acquired])\n",
    "\n",
    "print(\"iteration we acquire top COF = \", BO_iter_top_cof_acquired.item() + 1)\n",
    "print(\"accumulated cost up to observation of top COF = \", top_cof_acc_cost, \" [min]\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21be3bd",
   "metadata": {},
   "source": [
    "# Store Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe080e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfbo_res = dict({'acquired_set': acquired_set,\n",
    "                 'cost_acquired': build_cost(acquired_set) # name to be consistent with other notebooks\n",
    "                })\n",
    "\n",
    "with open('search_results/mfbo_results_with_EI.pkl', 'wb') as file:\n",
    "    pickle.dump(mfbo_res, file)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "873b9eed737f4a6d896d154d73024f53",
   "lastKernelId": "464ee535-3ea4-4199-a625-569df1ee6433"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
