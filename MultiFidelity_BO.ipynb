{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d087dd2",
   "metadata": {},
   "source": [
    "## System Description\n",
    "1. We have a set of COFs from a database. Each COF is characterized by a feature vector $$x_{COF} \\in \\mathcal{X} \\subset R^d$$ were d=14.\n",
    "\n",
    "\n",
    "2. We have **two different types** of simulations to calculate **the same material property $S_{Xe/Kr}$**. Therefore, we have a Single-Task/Objective \n",
    "$$argmax_{x_{COF} \\in \\mathcal{X}}[S_{Xe/Kr}(x_{COF})]$$\n",
    "\n",
    "3. Multi-Fidelity problem. \n",
    "    1. low-fidelity  => Henry coefficient calculation - MC integration: $S_{Xe/Kr} = \\frac{H_{Xe}}{H_{Kr}}$\n",
    "    2. high-fidelity => GCMC mixture simulation - 80:20 (Kr:Xe) at 298 K and 1.0 bar: $S_{Xe/Kr} = \\frac{n_{Xe} / n_{Kr}}{y_{Xe}/y_{Kr}}$\n",
    "\n",
    "\n",
    "3. We will initialize the system with a few COFs at **both** fidelities in order to initialize the Covariance Matrix.\n",
    "    1. The fist COF will be the one closest to the center of the normalized feature space\n",
    "    2. The rest will be chosen to maximize diversity of the training set\n",
    "\n",
    "\n",
    "4. Each surrogate model will **only train on data acquired at its level of fidelity** (Heterotopic data). $$X_{lf} \\neq X_{hf} \\subset X$$\n",
    "    1. We  use the augmented-EI (aEI) acquisition function from [here](https://link.springer.com/content/pdf/10.1007/s00158-005-0587-0.pdf)\n",
    "    2. Botorch GP surrogate model: [SingleTaskMultiFidelityGP](https://botorch.org/api/models.html#module-botorch.models.gp_regression_fidelity) or [FixedNoiseMultiFidelityGP](https://botorch.org/api/models.html#botorch.models.gp_regression_fidelity.FixedNoiseMultiFidelityGP)\n",
    "    3. Needed to use [this](https://botorch.org/api/optim.html#module-botorch.optim.fit) optimizer to correct matrix jitter\n",
    "    4. Helpful [tutorial](https://botorch.org/tutorials/discrete_multi_fidelity_bo) for a similar BoTorch Model used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "669c708a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "\n",
    "from botorch.models import SingleTaskMultiFidelityGP\n",
    "\n",
    "from botorch.models.transforms.outcome import Standardize\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch import fit_gpytorch_model\n",
    "\n",
    "from botorch.optim.fit import fit_gpytorch_torch # fix Cholecky jitter error\n",
    "\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "import pickle\n",
    "import h5py # for .jld2 files\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887fb687",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f94a155e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to: \"normalized\" => \"min_max\" \n",
    "normalization = \"normalized\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40c70241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total high-fidelity cost: 139887.66223703226 [min]\n",
      "total low-fidelity cost:  10076.305239888028 [min]\n",
      "average high-fidelity cost: 230.0783918372241 [min]\n",
      "average low-fidelity cost:  16.57287046034216 [min]\n",
      "average cost ratio:\t    13.444745568580501\n",
      "\n",
      "raw data - \n",
      "\tX: torch.Size([608, 14])\n",
      "\tfidelity: 0\n",
      "\t\ty: torch.Size([608])\n",
      "\t\tcost:  (608,)\n",
      "\tfidelity: 1\n",
      "\t\ty: torch.Size([608])\n",
      "\t\tcost:  (608,)\n",
      "\n",
      "Ensure features are normalized - \n",
      "max:\n",
      " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       dtype=torch.float64)\n",
      "min:\n",
      " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64)\n",
      "width:\n",
      " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       dtype=torch.float64)\n",
      "mean:\n",
      " tensor([0.2856, 0.5864, 0.5304, 0.3323, 0.0421, 0.1617, 0.6405, 0.6793, 0.0062,\n",
      "        0.1758, 0.0308, 0.0131, 0.0132, 0.0238], dtype=torch.float64)\n",
      "std:\n",
      " tensor([0.1586, 0.1787, 0.1896, 0.1512, 0.1150, 0.1916, 0.1631, 0.1354, 0.0631,\n",
      "        0.1126, 0.1026, 0.0935, 0.0740, 0.1043], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "#  load data: targets and features\n",
    "###\n",
    "file = h5py.File(\"targets_and_{}_features.jld2\".format(normalization), \"r\")\n",
    "\n",
    "\n",
    "# feature matrix\n",
    "X = torch.from_numpy(np.transpose(file[\"X\"][:]))\n",
    "# simulation data\n",
    "y = [torch.from_numpy(np.transpose(file[\"henry_y\"][:])), \n",
    "     torch.from_numpy(np.transpose(file[\"gcmc_y\"][:]))]\n",
    "# associated simulation costs\n",
    "cost = [np.transpose(file[\"henry_total_elapsed_time\"][:]), \n",
    "        np.transpose(file[\"gcmc_elapsed_time\"][:])]\n",
    "\n",
    "# total number of COFs in data set\n",
    "nb_COFs = X.shape[0] \n",
    "\n",
    "\n",
    "###\n",
    "#  load data: initializing COFs\n",
    "###\n",
    "init_cof_ids_file = pickle.load(open(\n",
    "                    'search_results/{}/initializing_cof_ids_{}.pkl'.format(normalization, normalization), \n",
    "                    'rb'))\n",
    "\n",
    "init_cof_ids = init_cof_ids_file['init_cof_ids']\n",
    "\n",
    "# total number of BO searches to run = number of initializing sets\n",
    "nb_runs = len(init_cof_ids)\n",
    "\n",
    "\n",
    "###\n",
    "#  print information about data\n",
    "###\n",
    "# cost\n",
    "print(\"total high-fidelity cost:\", sum(cost[1]).item(), \"[min]\")\n",
    "print(\"total low-fidelity cost: \", sum(cost[0]).item(), \"[min]\")\n",
    "print(\"average high-fidelity cost:\", np.mean(cost[1]), \"[min]\")\n",
    "print(\"average low-fidelity cost: \", np.mean(cost[0]), \"[min]\")\n",
    "print(\"average cost ratio:\\t   \", np.mean(cost[1] / cost[0]))\n",
    "# data shape\n",
    "print(\"\\nraw data - \\n\\tX:\", X.shape)\n",
    "for f in range(2):\n",
    "    print(\"\\tfidelity:\", f)\n",
    "    print(\"\\t\\ty:\", y[f].shape)\n",
    "    print(\"\\t\\tcost: \", cost[f].shape)\n",
    "# normalization \n",
    "print(\"\\nEnsure features are normalized - \")\n",
    "print(\"max:\\n\", torch.max(X, 0).values)\n",
    "print(\"min:\\n\", torch.min(X, 0).values)\n",
    "print(\"width:\\n\",torch.max(X, 0).values - torch.min(X, 0).values)\n",
    "print(\"mean:\\n\", torch.mean(X, 0))\n",
    "print(\"std:\\n\", torch.std(X, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f9ab59",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfab7a99",
   "metadata": {},
   "source": [
    "#### Post-Search Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fb35b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT GOING TO WORK WITH FRACTIONAL FIDELIY VALUES\n",
    "def calc_fidelity_fraction(acquired_set):\n",
    "    nb_iters = len(acquired_set)\n",
    "    fid_frac = np.zeros(nb_iters)\n",
    "    for i in range(1, nb_iters):\n",
    "        fid_frac[i] = sum(acquired_set[:, 0][:i+1]) / (i+1)\n",
    "    return fid_frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c591a6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_maxes_acquired(acquired_set):    \n",
    "    nb_iters = len(acquired_set)\n",
    "    y_maxes = np.zeros(nb_iters)\n",
    "    # we want the maximum y value (only high-fidelity) up to a given iteration\n",
    "    y_max = 0.0 # update this each iteration.\n",
    "    for i, (f_id, cof_id) in enumerate(acquired_set):\n",
    "        y_acq_this_iter = y[f_id][cof_id]\n",
    "        # i is iteration index\n",
    "        if f_id == 1 and y_acq_this_iter > y_max:\n",
    "            y_max = y_acq_this_iter # over-write max\n",
    "        y_maxes[i] = y_max \n",
    "    return y_maxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b3d16cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulated_cost(cost_acquired):\n",
    "    nb_iters = len(acquired_set)\n",
    "    accumulated_cost = np.zeros(nb_iters)\n",
    "    accumulated_cost[0] = cost_acquired[0]\n",
    "    for i in range(1, len(cost_acquired)):\n",
    "        accumulated_cost[i] = accumulated_cost[i-1] + cost_acquired[i]\n",
    "    return accumulated_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23cc873",
   "metadata": {},
   "source": [
    "#### Construct Initial Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f187575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_acquired_set(X, y, initializing_COFs, discrete_fidelities):\n",
    "#     cof_ids = diverse_set(X, nb_COFs_initialization) # np.array(ids_train)\n",
    "    return torch.tensor([[f_id, cof_id] for cof_id in initializing_COFs for f_id in discrete_fidelities])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "300bcf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct feature matrix of acquired points\n",
    "def build_X_train(acquired_set):\n",
    "    cof_ids = [a[1] for a in acquired_set]\n",
    "    f_ids = torch.tensor([a[0] for a in acquired_set])\n",
    "    return torch.cat((X[cof_ids, :], f_ids.unsqueeze(dim=-1)), dim=1)\n",
    "\n",
    "# construct output vector for acquired points\n",
    "def build_y_train(acquired_set):\n",
    "    return torch.tensor([y[f_id][cof_id] for f_id, cof_id in acquired_set]).unsqueeze(-1)\n",
    "\n",
    "# construct vector to track accumulated cost of acquired points\n",
    "def build_cost(acquired_set):\n",
    "    return torch.tensor([cost[f_id][cof_id] for f_id, cof_id in acquired_set]).unsqueeze(-1)\n",
    "\n",
    "# construct vector to track accumulated cost of acquired points\n",
    "def build_cost_fidelity(acquired_set, fidelity):\n",
    "    return torch.tensor([cost[f_id][cof_id] for f_id, cof_id in acquired_set if f_id == fidelity]).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec40fbc",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f1f9678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_initializing_functions(X, y):\n",
    "    ###\n",
    "    #  construct training sets\n",
    "    ###\n",
    "    # list of (cof_id, fid_id)'s\n",
    "    acquired_set = [[1, 10], [0, 3], [0, 4]]\n",
    "    \n",
    "    # Training Sets\n",
    "    X_train = build_X_train(acquired_set)\n",
    "    y_train = build_y_train(acquired_set)\n",
    "    \n",
    "    ###\n",
    "    #  test that the constructor functions are working properly\n",
    "    ###\n",
    "    assert np.allclose(X[10, :], X_train[0, :14])\n",
    "    assert X_train[0, 14] == 1\n",
    "    assert X_train[1, 14] == 0\n",
    "    assert y_train[0] == y[1][10] # y[fid_id][cof_id]\n",
    "    assert y_train[2] == y[0][4]\n",
    "    return\n",
    "\n",
    "test_initializing_functions(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d2ef9b",
   "metadata": {},
   "source": [
    "### Surrogate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b21c9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_surrogate_model(X_train, y_train, y_train_noise):\n",
    "    model = SingleTaskMultiFidelityGP(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        linear_truncated=False,\n",
    "        outcome_transform=Standardize(m=1), # m is the output dimension\n",
    "        data_fidelity=X_train.shape[1] - 1\n",
    "    )   \n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    fit_gpytorch_model(mll, optimizer=fit_gpytorch_torch)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a87aeb",
   "metadata": {},
   "source": [
    "### Acquisition Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6672eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate posterior mean and variance for a given fidelity\n",
    "def mu_sigma(model, X, fidelity):\n",
    "    f = torch.tensor((), dtype=torch.float64).new_ones((nb_COFs, 1)) * fidelity\n",
    "    X_f = torch.cat((X, f), dim=1) # last col is associated fidelity\n",
    "    f_posterior = model.posterior(X_f)\n",
    "    return f_posterior.mean.squeeze().detach().numpy(), np.sqrt(f_posterior.variance.squeeze().detach().numpy())\n",
    "\n",
    "# get the current best y-value of desired_fidelity in the acquired set\n",
    "def get_y_max(acquired_set, desired_fidelity):\n",
    "    return np.max([y[f_id][cof_id] for f_id, cof_id in acquired_set if f_id == desired_fidelity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ccf3d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#  efficient multi-fidelity correlation function\n",
    "#  corr(y at given fidelity, y at high-fidelity)\n",
    "#  (see notes)\n",
    "###\n",
    "def mfbo_correlation_function(model, X, fidelity):\n",
    "    # given fidelity\n",
    "    f   = torch.tensor((), dtype=torch.float64).new_ones((nb_COFs, 1)) * fidelity\n",
    "    X_f = torch.cat((X, f), dim=1) # last col is associated fidelity\n",
    "    \n",
    "    #  high-fidelity\n",
    "    hf   = torch.tensor((), dtype=torch.float64).new_ones((nb_COFs, 1)) \n",
    "    X_hf = torch.cat((X, hf), dim=1) # last col is associated fidelity\n",
    "\n",
    "    # combine into a single tensor\n",
    "    X_all_fid = torch.cat((X_f, X_hf), dim=0)\n",
    "    \n",
    "    # get variance for each fidelity\n",
    "    var_f = torch.flatten(model.posterior(X_f).variance)\n",
    "    var_hf = torch.flatten(model.posterior(X_hf).variance) # variance\n",
    "    \n",
    "    # posterior covariance \n",
    "    cov = torch.diag(model(X_all_fid).covariance_matrix[:X_f.size()[0], X_f.size()[0]:])\n",
    "    \n",
    "    corr = cov / (torch.sqrt(var_f) * torch.sqrt(var_hf))\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25b278ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#  cost ratio\n",
    "###\n",
    "def estimate_cost_ratio(fidelity, acquired_set):\n",
    "    avg_cost_f  = torch.mean(build_cost_fidelity(acquired_set, fidelity))\n",
    "    avg_cost_hf = torch.mean(build_cost_fidelity(acquired_set, 1))\n",
    "    cr = avg_cost_hf / avg_cost_f\n",
    "    return cr.item()\n",
    "\n",
    "###\n",
    "#  expected imrovement function, only uses hf\n",
    "###\n",
    "def EI_hf(model, X, acquired_set):\n",
    "    hf_mu, hf_sigma = mu_sigma(model, X, 1)\n",
    "    y_max = get_y_max(acquired_set, 1)\n",
    "    \n",
    "    z = (hf_mu - y_max) / hf_sigma\n",
    "    explore_term = hf_sigma * norm.pdf(z) \n",
    "    exploit_term = (hf_mu - y_max) * norm.cdf(z) \n",
    "    ei = explore_term + exploit_term\n",
    "    return np.maximum(ei, np.zeros(nb_COFs))\n",
    "\n",
    "###\n",
    "#  acquisition function\n",
    "###\n",
    "def acquisition_scores(model, X, fidelity, acquired_set):\n",
    "    # expected improvement for high-fidelity\n",
    "    ei = EI_hf(model, X, acquired_set) \n",
    "    \n",
    "    # augmenting functions\n",
    "    corr_f1_f0 = mfbo_correlation_function(model, X, fidelity)\n",
    "    \n",
    "    cr = estimate_cost_ratio(fidelity, acquired_set)\n",
    "\n",
    "    scores = torch.from_numpy(ei) * corr_f1_f0 * cr\n",
    "    return scores.detach().numpy()\n",
    "\n",
    "def in_acquired_set(f_id, cof_id, acquired_set):\n",
    "    for this_f_id, this_cof_id in acquired_set:\n",
    "        if this_cof_id == cof_id and this_f_id == f_id:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b84cbe",
   "metadata": {},
   "source": [
    "### Bayesian Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ae82c1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_Bayesian_optimization(nb_iterations, initializing_COFs, verbose=False):\n",
    "    assert nb_iterations > len(initializing_COFs)\n",
    "    ###\n",
    "    #  initialize system\n",
    "    ###\n",
    "    acquired_set = initialize_acquired_set(X, y, initializing_COFs, discrete_fidelities)\n",
    "    \n",
    "    ###\n",
    "    #  itterate through remaining budget using BO\n",
    "    ###\n",
    "    for i in range(nb_COFs_initialization * len(discrete_fidelities), nb_iterations): \n",
    "        print(\"BO iteration: \", i)\n",
    "        ###\n",
    "        #  construct training data (perform experiments)\n",
    "        ###\n",
    "        X_train = build_X_train(acquired_set)\n",
    "        y_train = build_y_train(acquired_set)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Initialization - \\n\")\n",
    "            print(\"\\tid acquired = \", [acq_[0].item() for acq_ in acquired_set])\n",
    "            print(\"\\tfidelity acquired = \", [acq_[1].item() for acq_ in acquired_set])\n",
    "            print(\"\\tcosts acquired = \", build_cost(acquired_set), \" [min]\")\n",
    "\n",
    "            print(\"\\n\\tTraining data:\\n\")\n",
    "            print(\"\\t\\t X train shape = \", X_train.shape)\n",
    "            print(\"\\t\\t y train shape = \", y_train.shape)\n",
    "            print(\"\\t\\t training feature vector = \\n\", X_train)\n",
    "        \n",
    "        ###\n",
    "        #  train Model\n",
    "        ###\n",
    "        model = train_surrogate_model(X_train, y_train)\n",
    "\n",
    "        ###\n",
    "        #  acquire new (COF, fidelity) not yet acquired.\n",
    "        ###\n",
    "        # entry (fid_id, cof_id) is the acquisition value for fidelity f_id and cof cof_id\n",
    "        the_acquisition_scores = np.array([acquisition_scores(model, X, f_id, acquired_set) for f_id in discrete_fidelities])\n",
    "        # overwrite acquired COFs/fidelities with negative infinity to not choose these.\n",
    "        for f_id, cof_id in acquired_set:\n",
    "            the_acquisition_scores[f_id, cof_id] = - np.inf\n",
    "        # select COF/fidelity with highest aquisition score.\n",
    "        f_id, cof_id = np.unravel_index(np.argmax(the_acquisition_scores), np.shape(the_acquisition_scores))\n",
    "        assert not in_acquired_set(f_id, cof_id, acquired_set)\n",
    "        # update acquired_set\n",
    "        acq = torch.tensor([[f_id, cof_id]], dtype=int)\n",
    "        acquired_set = torch.cat((acquired_set, acq))\n",
    "\n",
    "        ###\n",
    "        #  print useful info\n",
    "        ###\n",
    "        if verbose:\n",
    "            print(\"\\tacquired COF \", cof_id, \" at fidelity, \", f_id)\n",
    "            print(\"\\t\\ty = \", y[f_id][cof_id].item())\n",
    "            print(\"\\t\\tcost = \", cost[f_id][cof_id])\n",
    "        \n",
    "    return acquired_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2130854c",
   "metadata": {},
   "source": [
    "# Run MFBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4c4d0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#  construct initial inputs\n",
    "###\n",
    "# discrete_fidelities = [0, 1] # set of discrete fidelities to select from\n",
    "discrete_fidelities = [1/3, 2/3] # set of discrete fidelities to select from\n",
    "\n",
    "nb_COFs_initialization = 3   # at each fidelity, number of COFs to initialize with\n",
    "nb_iterations = 150          # BO budget, includes initializing COFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d66fd66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run #: 0\n",
      "BO iteration:  6\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_904741/1181141405.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m#  run BO search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0macquired_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_Bayesian_optimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializing_COFs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_904741/1532370723.py\u001b[0m in \u001b[0;36mrun_Bayesian_optimization\u001b[0;34m(nb_iterations, initializing_COFs, verbose)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m#  construct training data (perform experiments)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_X_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macquired_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_y_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macquired_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_904741/70597796.py\u001b[0m in \u001b[0;36mbuild_X_train\u001b[0;34m(acquired_set)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcof_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0macquired_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mf_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0macquired_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcof_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# construct output vector for acquired points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "###\n",
    "#  run search\n",
    "###\n",
    "for j, initializing_COFs in enumerate(init_cof_ids[:10]): \n",
    "    # check the length of each initializing set\n",
    "    assert len(initializing_COFs) == nb_COFs_initialization\n",
    "    print(\"run #: {}\".format(j))\n",
    "    \n",
    "    # start timer for BO run\n",
    "    start_time = time.time()\n",
    "    ###\n",
    "    #  run BO search\n",
    "    ###\n",
    "    acquired_set = run_Bayesian_optimization(nb_iterations, initializing_COFs)\n",
    "    \n",
    "    ###\n",
    "    #  post-run analysis\n",
    "    ###\n",
    "    elapsed_time  = time.time() - start_time\n",
    "    print(\"elapsed_time:\\t\", elapsed_time / 60, \" min.\")\n",
    "    y_acquired    = build_y_train(acquired_set)\n",
    "    y_maxes_acq   = get_y_maxes_acquired(acquired_set.detach().numpy())\n",
    "    fid_fraction  = calc_fidelity_fraction(acquired_set.detach().numpy())\n",
    "    cost_acquired = build_cost(acquired_set)\n",
    "    acc_cost      = accumulated_cost(cost_acquired.flatten().detach().numpy())\n",
    "    \n",
    "    ###\n",
    "    # look at unique COFs acquired\n",
    "    ###\n",
    "    # cof_ids_acquired = torch.tensor([acq[1] for acq in acquired_set])\n",
    "    n_unique_cofs_acquired = len(np.unique([acq[1] for acq in acquired_set]))\n",
    "    print(\"total number of unique COFs acquired\", n_unique_cofs_acquired)\n",
    "\n",
    "    ###\n",
    "    #  Iterations until top COF and accumulated \n",
    "    ###\n",
    "    cof_id_with_max_selectivity = np.argmax(y[1])\n",
    "    BO_iter_top_cof_acquired = float(\"inf\") # dummy \n",
    "    for i, (f_id, cof_id) in enumerate(acquired_set):\n",
    "        if cof_id == cof_id_with_max_selectivity and f_id == 1:\n",
    "            BO_iter_top_cof_acquired = i\n",
    "            print(\"woo, top COF acquired!\")\n",
    "            print(\"iteration we acquire top COF = \", BO_iter_top_cof_acquired) \n",
    "            break\n",
    "        elif i == len(acquired_set)-1:\n",
    "            print(\"oh no, top COF not acquired!\")\n",
    "\n",
    "\n",
    "    top_cof_acc_cost = sum(build_cost(acquired_set)[:BO_iter_top_cof_acquired])\n",
    "    print(\"accumulated cost up to observation of top COF = \", top_cof_acc_cost.item(), \" [min]\")\n",
    "    \n",
    "    ###\n",
    "    #  store results\n",
    "    ###\n",
    "    mfbo_res = dict({'acquired_set': acquired_set.detach().numpy(),\n",
    "                     'y_acquired': y_acquired.detach().numpy(),\n",
    "                     'y_max_acquired': y_maxes_acq,\n",
    "                     'fidelity_fraction': fid_fraction,\n",
    "                     'cost_acquired': cost_acquired.flatten().detach().numpy(),\n",
    "                     'accumulated_cost': acc_cost / 60,\n",
    "                     'nb_COFs_initialization': nb_COFs_initialization,\n",
    "                     'BO_iter_top_cof_acquired': BO_iter_top_cof_acquired,\n",
    "                     'elapsed_time (min)':  elapsed_time / 60\n",
    "                    })\n",
    "\n",
    "    with open('search_results/{}/mfbo_results/mfbo_results_run_{}.pkl'.format(normalization, j), 'wb') as file:\n",
    "        pickle.dump(mfbo_res, file)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "873b9eed737f4a6d896d154d73024f53",
   "lastKernelId": "464ee535-3ea4-4199-a625-569df1ee6433"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "23d6f7179e0bc929ef338139025b84bf12b8362d4f462c6c3a682d200e09aab4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
