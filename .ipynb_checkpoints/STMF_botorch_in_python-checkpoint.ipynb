{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8613aa76",
   "metadata": {},
   "source": [
    "## System Description\n",
    "1. We have a set of COFs from a database. Each COF is characterized by a feature vector $$x_{COF} \\in X \\subset R^d$$ were d=14.\n",
    "\n",
    "\n",
    "2. We have **two different types** of simulations to calculate **the same material property $S_{Xe/Kr}$**. Therefore, we have a Single-Task/Objective (find the material with the optimal selevtivity), Multi-Fidelity problem. \n",
    "    1. low-fidelity  = Henry coefficient calculation - MC integration - cost=1\n",
    "    2. high-fidelity = GCMC mixture simulation - 80:20 (Kr:Xe) at 298 K and 1.0 bar - cost=30\n",
    "\n",
    "\n",
    "3. We will initialize the system with *two* COFs at both fidelities in order to initialize the Covariance Matrix.\n",
    "    - The fist COF will be the one closest to the center of the normalized feature space\n",
    "    - The second COF will be chosen at random\n",
    "\n",
    "\n",
    "4. Each surrogate model will **only train on data acquired at its level of fidelity** (Heterotopic data). $$X_{lf} \\neq X_{hf} \\subset X$$\n",
    "    1. We could use the augmented EI acquisition function from [here](https://link.springer.com/content/pdf/10.1007/s00158-005-0587-0.pdf)\n",
    "    2. We could use a naive implementation of the [misoKG](https://papers.nips.cc/paper/2017/file/df1f1d20ee86704251795841e6a9405a-Paper.pdf) acquisition function\n",
    "    3. **OR** we could use the acquisition function from the [tutorial](https://botorch.org/tutorials/discrete_multi_fidelity_bo)\n",
    "\n",
    "\n",
    "5. **kernel model**: \n",
    "    1.  We need a Gaussian Process (GP) that will give a *correlated output for each fidelity* i.e. we need a vector-valued kernel\n",
    "    2. Given the *cost aware* acquisition function, we anticipate the number of training points at each fidelity *will not* be equal (asymmetric scenario) $$n_{lf} > n_{hf}$$\n",
    "        - perhaps we can force the symmetric case, $n_{lf} = n_{hf} = n$, if we can include `missing` or `empty` entries in the training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7679eb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "from botorch.models import SingleTaskMultiFidelityGP\n",
    "from botorch.models.transforms.outcome import Standardize\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch import fit_gpytorch_model\n",
    "from scipy.stats import norm\n",
    "import math \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aa45b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw data - \n",
      "\tX: torch.Size([608, 14])\n",
      "\tfidelity: 0\n",
      "\t\ty: torch.Size([608])\n",
      "\t\tcost:  torch.Size([608])\n",
      "\tfidelity: 1\n",
      "\t\ty: torch.Size([608])\n",
      "\t\tcost:  torch.Size([608])\n",
      "\n",
      "Ensure features are normalized - \n",
      "max:\n",
      " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       dtype=torch.float64)\n",
      "min:\n",
      " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "#  Load Data\n",
    "###\n",
    "f = h5py.File(\"targets_and_normalized_features.jld2\", \"r\")\n",
    "# feature matrix\n",
    "X = torch.from_numpy(np.transpose(f[\"X\"][:]))\n",
    "# simulation data\n",
    "y = [torch.from_numpy(np.transpose(f[\"henry_y\"][:])), \n",
    "     torch.from_numpy(np.transpose(f[\"gcmc_y\"][:]))]\n",
    "# associated simulation costs\n",
    "cost = [torch.from_numpy(np.transpose(f[\"henry_total_elapsed_time\"][:])), \n",
    "        torch.from_numpy(np.transpose(f[\"gcmc_elapsed_time\"][:]))]\n",
    "\n",
    "print(\"raw data - \\n\\tX:\", X.shape)\n",
    "for f in range(2):\n",
    "    print(\"\\tfidelity:\", f)\n",
    "    print(\"\\t\\ty:\", y[f].shape)\n",
    "    print(\"\\t\\tcost: \", cost[f].shape)\n",
    "    \n",
    "    \n",
    "print(\"\\nEnsure features are normalized - \")\n",
    "print(\"max:\\n\", torch.max(X, 0).values)\n",
    "print(\"min:\\n\", torch.min(X, 0).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f84bd4",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "#### Construct Initial Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c0266c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find COF closest to the center of feature space\n",
    "def get_initializing_COF(X):\n",
    "    # center of feature space\n",
    "    feature_center = np.ones(X.shape[1]) * 0.5\n",
    "    # max possible distance between normalized features\n",
    "    min_dist = np.inf\n",
    "    min_id = 0 # dummy id \n",
    "    for i in range(nb_COFs):\n",
    "        dist = sum((X[i] - feature_center) * (X[i] - feature_center)).item()\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            min_id = i\n",
    "    return min_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e608c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct feature matrix of acquired points\n",
    "def build_X_train(ids_acquired, fidelity_acquired):\n",
    "    return torch.cat((X[ids_acquired, :], fidelity_acquired), dim=1)\n",
    "\n",
    "# construct output vector for acquired points\n",
    "def build_y_train(ids_acquired, fidelity_acquired):\n",
    "    train_y = torch.tensor((), dtype=torch.float64).new_zeros((ids_acquired.shape[0], 1))\n",
    "    for i, fid in enumerate(fidelity_acquired):\n",
    "        train_y[i][0] = y[fid][ids_acquired[i]]\n",
    "    return train_y\n",
    "\n",
    "# construct vector to track accumulated cost of acquired points\n",
    "def build_cost(ids_acquired, fidelity_acquired):\n",
    "    costs_acquired = torch.tensor((), dtype=torch.float64).new_zeros((ids_acquired.shape[0], 1))\n",
    "    for i, fid in enumerate(fidelity_acquired):\n",
    "        costs_acquired[i][0] = cost[fid][ids_acquired[i]]\n",
    "    return costs_acquired"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d57717a",
   "metadata": {},
   "source": [
    "#### Surrogate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73c5e7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_surrogate_model(X_train, y_train):\n",
    "    model = SingleTaskMultiFidelityGP(\n",
    "        X_train, \n",
    "        y_train, \n",
    "        outcome_transform=Standardize(m=1), # m is the output dimension\n",
    "        data_fidelity=X_train.shape[1] - 1\n",
    "    )   \n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    fit_gpytorch_model(mll)\n",
    "    return mll, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce88b3c",
   "metadata": {},
   "source": [
    "#### Acquisition Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70bb7e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate posterior mean and variance\n",
    "def mu_sigma(model, X, fidelity):\n",
    "    f = torch.tensor((), dtype=torch.float64).new_ones((nb_COFs, 1)) * fidelity\n",
    "    X_f = torch.cat((X, f), dim=1) # last col is associated fidelity\n",
    "    f_posterior = model.posterior(X_f)\n",
    "    return f_posterior.mean.squeeze().detach().numpy(), f_posterior.variance.squeeze().detach().numpy()\n",
    "\n",
    "# get the current \"effective best solution\" \n",
    "def get_y_max(ids_acquired, fidelity_acquired, desired_fidelity):\n",
    "    y_max = torch.tensor((), dtype=torch.float64).new_zeros(1)\n",
    "    for i, fid in enumerate(fidelity_acquired):\n",
    "        if (fid == desired_fidelity) & (y[fid][ids_acquired[i]] > y_max):\n",
    "            y_max = y[fid][ids_acquired[i]]\n",
    "    return y_max.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e03dd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha_1 = corr(f^p_l(x), f^p_m(x))\n",
    "# correlation of the posterior dists. between given fidelity the high-fidelity\n",
    "\n",
    "# check out \"marginalization and Condtioning\"\n",
    "# X = [f(x, s), f(x', s')] the two inputs we are interested in\n",
    "# Y = [f(x_1, s_1), ..., f(x_n, s_n)] the data collected so far\n",
    "# https://distill.pub/2019/visual-exploration-gaussian-processes/\n",
    "\n",
    "def augment_1(model, X, fidelity, ids_acquired, fidelity_acquired):\n",
    "    if fidelity == 1:\n",
    "        return 1.0\n",
    "    # get posterior at each fidelity\n",
    "    f_mu, f_sigma   = mu_sigma(model, X, fidelity) \n",
    "    hf_mu, hf_sigma = mu_sigma(model, X, 1)\n",
    "#     K = model.covar_module(X_train).evaluate()\n",
    "#     K_inv = torch.inverse(K)\n",
    "    # get \"best linear predictor\" at each fidelity\n",
    "    f_y_max  = get_y_max(ids_acquired, fidelity_acquired, fidelity)\n",
    "    hf_y_max = get_y_max(ids_acquired, fidelity_acquired, 1)\n",
    "    # calculate correlation\n",
    "    a1 = ((f_mu - f_y_max) * (hf_mu - hf_y_max)) / (f_sigma * hf_sigma)\n",
    "    return a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9890f8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_fidelity_correlation(model, X, fidelity, ids_acquired, fidelity_acquired):\n",
    "    # get covariance matrix of acquired data points\n",
    "    K = model.covar_module(X_train).evaluate() \n",
    "    K_inv = torch.inverse(K) # take the inverse\n",
    "    \n",
    "    # get posterior for fidelity f\n",
    "    f   = torch.tensor((), dtype=torch.float64).new_ones((nb_COFs, 1)) * fidelity\n",
    "    X_f = torch.cat((X, f), dim=1) # last col is associated fidelity\n",
    "    f_posterior = model.posterior(X_f)\n",
    "    \n",
    "    # get posterior for high-fidelity\n",
    "    hf = torch.tensor((), dtype=torch.float64).new_ones((nb_COFs, 1)) \n",
    "    X_hf = torch.cat((X, hf), dim=1) # last col is associated fidelity\n",
    "    hf_posterior = model.posterior(X_hf)\n",
    "    \n",
    "    # Compute the covariance between X_hf and X_f using kernel\n",
    "    fwd = model.covar_module.forward(X_hf, X_f).evaluate()\n",
    "    \n",
    "    kern_s = model.covar_module.forward(X_f, X_train).evaluate()\n",
    "    \n",
    "    kern_s1 = model.covar_module.forward(X_hf, X_train).evaluate()\n",
    "    \n",
    "    a = torch.matmul(torch.matmul(kern_s, K_inv), torch.t(kern_s1))\n",
    "    \n",
    "    cov = fwd - a\n",
    "    \n",
    "    # calculate the correlation\n",
    "    corr = cov / (torch.sqrt(f_posterior.variance) * torch.sqrt(hf_posterior.variance))\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f86f49bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the cost ratio\n",
    "def cost_ratio(fidelity, fidelity_acquired, costs_acquired):\n",
    "    avg_cost_f  = torch.mean(costs_acquired[fidelity_acquired == fidelity]).item()\n",
    "    avg_cost_hf = torch.mean(costs_acquired[fidelity_acquired == 1]).item()\n",
    "    return avg_cost_hf / avg_cost_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "939d204e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected Imrovement function, only use hf\n",
    "def EI_hf(model, X, ids_acquired, fidelity_acquired):\n",
    "    hf_mu, hf_sigma = mu_sigma(model, X, 1)\n",
    "    y_max = get_y_max(ids_acquired, fidelity_acquired, 1)\n",
    "    \n",
    "    z = (hf_mu - y_max) / hf_sigma\n",
    "    explore_term = hf_sigma * norm.pdf(z)\n",
    "    exploit_term = (hf_mu - y_max) * norm.cdf(z)\n",
    "    ei = explore_term + exploit_term\n",
    "    return np.maximum(ei, np.zeros(nb_COFs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23a3c955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. We're going to get a sorted list of propoed acquisition values at each fidelity \n",
    "# 2. Then, we will determine which value is highest and from which fidelity it was derived\n",
    "# 3. Add that data point and associated fidelity to the training data\n",
    "# 4. Retrian model and repreat process\n",
    "def acquisition(model, X, fidelity, ids_acquired, fidelity_acquired, costs_acquired):\n",
    "    # expected improvement for high-fidelity\n",
    "    ei = EI_hf(model, X, ids_acquired, fidelity_acquired) \n",
    "    \n",
    "    # augmenting functions\n",
    "#     a1 = augment_1(model, X, fidelity, ids_acquired, fidelity_acquired)\n",
    "    a1 = multi_fidelity_correlation(model, X, fidelity, ids_acquired, fidelity_acquired)\n",
    "    a2 = 1.0 # no systematic random error noise \n",
    "    a3 = cost_ratio(fidelity, fidelity_acquired, costs_acquired)\n",
    "\n",
    "    acquisition_values = torch.matmul(a1, torch.from_numpy(ei)) * a2 * a3\n",
    "#     acquisition_values = ei * a1 * a2 * a3\n",
    "    \n",
    "    return acquisition_values, acquisition_values.argsort(descending=True) # sort in descending order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd075cf",
   "metadata": {},
   "source": [
    "## Run MFBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02da01a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization - \n",
      "\n",
      "\tid acquired =  25\n",
      "\tfidelity acquired =  1\n",
      "\tcosts acquired =  399.7576660990715  [min]\n",
      "\tTraining data:\n",
      "\n",
      "\t\t X train shape =  torch.Size([1, 15])\n",
      "\t\t y train shape =  torch.Size([1, 1])\n",
      "\t\t training feature vector = \n",
      " tensor([0.1500, 0.4533, 0.1088, 0.5523, 0.4387, 0.1463, 0.3480, 0.2643, 0.0000,\n",
      "        0.1769, 0.2237, 0.0000, 0.0000, 0.3471, 1.0000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "#  construct initial inputs\n",
    "###\n",
    "nb_COFs = X.shape[0] # total number of COFs data points \n",
    "nb_iterations = 100  # BO budget\n",
    "\n",
    "ids_acquired = torch.ones(1, dtype=int) * get_initializing_COF(X)\n",
    "fidelity_acquired = torch.ones((1, 1), dtype=int) # start with high-fidelity\n",
    "costs_acquired = build_cost(ids_acquired, fidelity_acquired)\n",
    "\n",
    "X_train = build_X_train(ids_acquired, fidelity_acquired)\n",
    "y_train = build_y_train(ids_acquired, fidelity_acquired)\n",
    "\n",
    "print(\"Initialization - \\n\")\n",
    "print(\"\\tid acquired = \", ids_acquired.item())\n",
    "print(\"\\tfidelity acquired = \", fidelity_acquired.item())\n",
    "print(\"\\tcosts acquired = \", costs_acquired.item(), \" [min]\")\n",
    "\n",
    "print(\"\\tTraining data:\\n\")\n",
    "print(\"\\t\\t X train shape = \", X_train.shape)\n",
    "print(\"\\t\\t y train shape = \", y_train.shape)\n",
    "print(\"\\t\\t training feature vector = \\n\", X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e901d0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "track_acq_vals = True\n",
    "if track_acq_vals:\n",
    "    stored_acquisition_values = torch.zeros(nb_iterations, dtype=float)\n",
    "\n",
    "\n",
    "###\n",
    "#  Evaluate acquisition function at each fidelity\n",
    "###\n",
    "for i in range(1, nb_iterations):\n",
    "    mll, model = train_surrogate_model(X_train, y_train)\n",
    "    \n",
    "    ###\n",
    "    #  Acquisition Function\n",
    "    ###\n",
    "    lf_acquisition_values, lf_acquisition_sorted = acquisition(model, X, 0, ids_acquired, \n",
    "                                                               fidelity_acquired, costs_acquired)\n",
    "    hf_acquisition_values, hf_acquisition_sorted = acquisition(model, X, 1, ids_acquired, \n",
    "                                                               fidelity_acquired, costs_acquired)\n",
    "    \n",
    "    if 0 not in fidelity_acquired: # if no low-fidelity points, force the choice\n",
    "        for id_ in lf_acquisition_sorted:\n",
    "            if not id_.item() in ids_acquired:\n",
    "                lf_id_max_aquisition = id_.item()\n",
    "                break\n",
    "        ids_acquired = torch.cat((ids_acquired, torch.ones(1, dtype=int) * lf_id_max_aquisition))\n",
    "        fidelity_acquired = torch.cat((fidelity_acquired, torch.zeros((1, 1), dtype=int)))\n",
    "        acquisition_value = lf_acquisition_values[lf_id_max_aquisition]\n",
    "    else:\n",
    "        for id_ in lf_acquisition_sorted:\n",
    "            if not id_ in ids_acquired:\n",
    "                lf_id_max_aquisition = id_.item()\n",
    "                break\n",
    "        for id_ in hf_acquisition_sorted:\n",
    "            if not id_ in ids_acquired:\n",
    "                hf_id_max_aquisition = id_.item()\n",
    "                break\n",
    "        if lf_acquisition_values[lf_id_max_aquisition] >= hf_acquisition_values[hf_id_max_aquisition]:\n",
    "            ids_acquired = torch.cat((ids_acquired, torch.ones(1, dtype=int) * lf_id_max_aquisition))\n",
    "            fidelity_acquired = torch.cat((fidelity_acquired, torch.zeros((1, 1), dtype=int)))\n",
    "            acquisition_value = lf_acquisition_values[lf_id_max_aquisition]\n",
    "        else:\n",
    "            ids_acquired = torch.cat((ids_acquired, torch.ones(1, dtype=int) * hf_id_max_aquisition))\n",
    "            fidelity_acquired = torch.cat((fidelity_acquired, torch.ones((1, 1), dtype=int)))\n",
    "            acquisition_value = hf_acquisition_values[hf_id_max_aquisition]\n",
    "    \n",
    "    # update training sets and cost\n",
    "    X_train = build_X_train(ids_acquired, fidelity_acquired)\n",
    "    y_train = build_y_train(ids_acquired, fidelity_acquired)\n",
    "    costs_acquired = build_cost(ids_acquired, fidelity_acquired)\n",
    "    \n",
    "    # track acquisition value\n",
    "    if track_acq_vals:\n",
    "        stored_acquisition_values[i] = acquisition_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2565d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids_acquired\n",
    "# fidelity_acquired\n",
    "# costs_acquired\n",
    "# stored_acquisition_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b6a7b6",
   "metadata": {},
   "source": [
    "## PLOTS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00d7608",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.zeros(len(ids_acquired), dtype=int)\n",
    "max_selectivity  = np.zeros(len(ids_acquired), dtype=float)\n",
    "net_cost  = np.zeros(len(ids_acquired), dtype=float)\n",
    "\n",
    "nb_COFs_initialization = 1\n",
    "fid_color = []\n",
    "\n",
    "for i in range(len(ids_acquired)):\n",
    "    ittration = i + nb_COFs_initialization\n",
    "    index[i] = ittration\n",
    "    net_cost[i] = sum(costs_acquired[:i])\n",
    "    if fidelity_acquired[i] == 1:\n",
    "        max_selectivity[i]  = max(y[1][ids_acquired[:ittration]])\n",
    "        fid_color.append(\"tab:red\")\n",
    "    else:\n",
    "        max_selectivity[i]  = max(y[0][ids_acquired[:ittration]])\n",
    "        fid_color.append(\"tab:cyan\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbdbbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max(y[1])\n",
    "# max(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8deb77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.axvline(x=nb_COFs_initialization, label=\"initialization\", color=\"k\", alpha=0.5, linestyle=\"--\", lw=2)\n",
    "plt.axhline(y=max(y[1]), label=\"global maximm\", color=\"tab:green\", ls=\"--\", lw=1.5)\n",
    "\n",
    "plt.bar(index, max_selectivity, 0.5, label=\"fidelity\", align=\"center\", \n",
    "        color=fid_color, alpha=0.5, edgecolor=\"k\", lw=0.5)\n",
    "\n",
    "plt.title(\"Multi-fidelity Seach Efficiency\")\n",
    "plt.xlabel(\"# evaluated COFs\", fontsize=12)\n",
    "plt.ylabel(\"Maximum \" + \"$S_{Xe/Kr}$\" + \" among acquired COFs\", fontsize=12)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e8a429",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.bar(index, stored_acquisition_values, width=0.5, align=\"center\", color=fid_color, \n",
    "        alpha=0.5, edgecolor=\"k\", lw=1.0)\n",
    "plt.xlabel(\"itteration\")\n",
    "plt.ylabel(\"acquisition values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71609bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.bar(index, costs_acquired, color=fid_color, alpha=0.5, edgecolor=\"k\", lw=1.0)\n",
    "\n",
    "plt.plot(index - 1, net_cost, color=\"tab:green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098453a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
