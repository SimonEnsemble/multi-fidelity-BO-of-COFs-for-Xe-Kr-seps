{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8613aa76",
   "metadata": {},
   "source": [
    "## System Description\n",
    "1. We have a set of COFs from a database. Each COF is characterized by a feature vector $$x_{COF} \\in X \\subset R^d$$ were d=14.\n",
    "\n",
    "\n",
    "2. We have **two different types** of simulations to calculate **the same material property $S_{Xe/Kr}$**. Therefore, we have a Single-Task/Objective (find the material with the optimal selevtivity), Multi-Fidelity problem. \n",
    "    1. low-fidelity  = Henry coefficient calculation - MC integration - cost=1\n",
    "    2. high-fidelity = GCMC mixture simulation - 80:20 (Kr:Xe) at 298 K and 1.0 bar - cost=30\n",
    "\n",
    "\n",
    "3. We will initialize the system with *two* COFs at both fidelities in order to initialize the Covariance Matrix.\n",
    "    - The fist COF will be the one closest to the center of the normalized feature space\n",
    "    - The second COF will be chosen at random\n",
    "\n",
    "\n",
    "4. Each surrogate model will **only train on data acquired at its level of fidelity** (Heterotopic data). $$X_{lf} \\neq X_{hf} \\subset X$$\n",
    "    1. We are using the augmented EI acquisition function from [here](https://link.springer.com/content/pdf/10.1007/s00158-005-0587-0.pdf)\n",
    "\n",
    "\n",
    "5. **kernel model**: \n",
    "    1.  We need a Gaussian Process (GP) that will give a *correlated output for each fidelity* i.e. we need a vector-valued kernel\n",
    "    2. Given the *cost aware* acquisition function, we anticipate the number of training points at each fidelity *will not* be equal (asymmetric scenario) $$n_{lf} > n_{hf}$$\n",
    "        - perhaps we can force the symmetric case, $n_{lf} = n_{hf} = n$, if we can include `missing` or `empty` entries in the training sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b61313",
   "metadata": {},
   "source": [
    "### Strategy\n",
    "1. Implement SingleTaskMultiFidelity Gp\n",
    "2. Get augmented EI working\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7679eb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "from botorch.models import SingleTaskMultiFidelityGP\n",
    "from botorch.models.transforms.outcome import Standardize\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch import fit_gpytorch_model\n",
    "\n",
    "from scipy.stats import norm\n",
    "import math \n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d83c705d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw data - \n",
      "X: torch.Size([608, 14])\n",
      "henry_y: torch.Size([608])\n",
      "gcmc_y:  torch.Size([608])\n",
      "input data - \n",
      "train_y: torch.Size([7, 1]) \n",
      "train_x:  torch.Size([7, 15])\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "#  import data\n",
    "###\n",
    "f = h5py.File(\"targets_and_normalized_features.jld2\", \"r\")\n",
    "\n",
    "X = torch.from_numpy(np.transpose(f[\"X\"][:]))\n",
    "henry_y = torch.from_numpy(np.transpose(f[\"henry_y\"][:]))\n",
    "gcmc_y  = torch.from_numpy(np.transpose(f[\"gcmc_y\"][:]))\n",
    "print(\"raw data - \\nX:\", X.shape)\n",
    "print(\"henry_y:\", henry_y.shape)\n",
    "print(\"gcmc_y: \", gcmc_y.shape)\n",
    "\n",
    "###\n",
    "#  construct initial inputs\n",
    "#  1. get initial points\n",
    "#  2. standardize outputs\n",
    "#  3. stack into tensor\n",
    "###\n",
    "nb_COFs = henry_y.shape[0] # total number of COFs data points \n",
    "nb_COFs_initialization = 7 # number of COFs to initialize with\n",
    "ids_acquired = np.random.choice(np.arange((nb_COFs)), size=nb_COFs_initialization, replace=False)\n",
    "\n",
    "fidelities = torch.tensor([0.1, 1.0]) # assign fidelities (arbitrary?)\n",
    "fid_acquired = torch.randint(2, (nb_COFs_initialization, 1))\n",
    "train_f = fidelities[fid_acquired] # selected fidelity of training points\n",
    "\n",
    "train_x = X[ids_acquired, :] # torch.Size([nb_COFs_initialization, 14])\n",
    "train_x_full = torch.cat((train_x, train_f), dim=1) # last col is associated fidelity\n",
    "\n",
    "\n",
    "train_y = torch.tensor((), dtype=torch.float64).new_zeros((ids_acquired.shape[0], 1))\n",
    "for i, fid in enumerate(fid_acquired):\n",
    "    if fid == 0:\n",
    "        train_y[i][0] = henry_y[ids_acquired[i]]\n",
    "    else:\n",
    "        train_y[i][0] = gcmc_y[ids_acquired[i]]\n",
    "        \n",
    "print(\"input data - \\ntrain_y:\", train_y.shape, \"\\ntrain_x: \", train_x_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02f59c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([608, 1, 14])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_unsqueezed = X.unsqueeze(1)\n",
    "X_unsqueezed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe0a8504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExactMarginalLogLikelihood(\n",
       "  (likelihood): GaussianLikelihood(\n",
       "    (noise_covar): HomoskedasticNoise(\n",
       "      (noise_prior): GammaPrior()\n",
       "      (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "    )\n",
       "  )\n",
       "  (model): SingleTaskMultiFidelityGP(\n",
       "    (likelihood): GaussianLikelihood(\n",
       "      (noise_covar): HomoskedasticNoise(\n",
       "        (noise_prior): GammaPrior()\n",
       "        (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "      )\n",
       "    )\n",
       "    (mean_module): ConstantMean()\n",
       "    (covar_module): ScaleKernel(\n",
       "      (base_kernel): LinearTruncatedFidelityKernel(\n",
       "        (raw_power_constraint): Positive()\n",
       "        (power_prior): GammaPrior()\n",
       "        (covar_module_unbiased): MaternKernel(\n",
       "          (lengthscale_prior): GammaPrior()\n",
       "          (raw_lengthscale_constraint): Positive()\n",
       "          (distance_module): Distance()\n",
       "        )\n",
       "        (covar_module_biased): MaternKernel(\n",
       "          (lengthscale_prior): GammaPrior()\n",
       "          (raw_lengthscale_constraint): Positive()\n",
       "          (distance_module): Distance()\n",
       "        )\n",
       "      )\n",
       "      (outputscale_prior): GammaPrior()\n",
       "      (raw_outputscale_constraint): Positive()\n",
       "    )\n",
       "    (outcome_transform): Standardize()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###\n",
    "#  construct surrogate model\n",
    "###\n",
    "def initialize_model(train_x, train_obj):\n",
    "    model = SingleTaskMultiFidelityGP(\n",
    "        train_x, \n",
    "        train_obj, \n",
    "        outcome_transform=Standardize(m=1), # m is the output dimension\n",
    "        data_fidelity=14\n",
    "    )   \n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    return mll, model\n",
    "\n",
    "mll, model = initialize_model(train_x_full, train_y)\n",
    "fit_gpytorch_model(mll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "575e321e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SingleTaskMultiFidelityGP(\n",
       "  (likelihood): GaussianLikelihood(\n",
       "    (noise_covar): HomoskedasticNoise(\n",
       "      (noise_prior): GammaPrior()\n",
       "      (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "    )\n",
       "  )\n",
       "  (mean_module): ConstantMean()\n",
       "  (covar_module): ScaleKernel(\n",
       "    (base_kernel): LinearTruncatedFidelityKernel(\n",
       "      (raw_power_constraint): Positive()\n",
       "      (power_prior): GammaPrior()\n",
       "      (covar_module_unbiased): MaternKernel(\n",
       "        (lengthscale_prior): GammaPrior()\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "        (distance_module): Distance()\n",
       "      )\n",
       "      (covar_module_biased): MaternKernel(\n",
       "        (lengthscale_prior): GammaPrior()\n",
       "        (raw_lengthscale_constraint): Positive()\n",
       "        (distance_module): Distance()\n",
       "      )\n",
       "    )\n",
       "    (outputscale_prior): GammaPrior()\n",
       "    (raw_outputscale_constraint): Positive()\n",
       "  )\n",
       "  (outcome_transform): Standardize()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bebe3d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#  Acquisition function\n",
    "###\n",
    "# def u():\n",
    "#     # utility function: u(x) = -f̂ₘ(x) - csₘ(x) \n",
    "#     c = 1.0\n",
    "#     sm = model.posterior().variance\n",
    "#     x_star = \n",
    "#     return x_star\n",
    "\n",
    "# def α1():\n",
    "#     # corr[fₗᵖ(x), fₘᵖ(x)]\n",
    "#     return\n",
    "\n",
    "# def α2():\n",
    "#     #\n",
    "#     return\n",
    "\n",
    "# def α3(cm, cl):\n",
    "#     # cost ratio: cₘ/cₗ\n",
    "#     cost_ratio = cm / cl\n",
    "#     return cost_ratio\n",
    "\n",
    "# EI = (f̂ₘ(x*) - f̂ₘ(m))Φ(z) + sₘ(x)ϕ(z)\n",
    "# z = (f̂ₘ(x*) - f̂ₘ(x)) / sₘ(x)\n",
    "# where sₘ(x) = sqrt(cov[fₘᵖ(x), fₘᵖ(x)]) i.e. MSE,\n",
    "# and x* is the \"efective best solution\" -> x* = argmax_{x in {xᵢ; i=1,..,n}}[u(x)]\n",
    "# s.t. u(x) = -f̂ₘ(x) - csₘ(x) is the utility function, c=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71591252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c61cd6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f468870",
   "metadata": {},
   "source": [
    "**Tutorial** [link](https://botorch.org/tutorials/discrete_multi_fidelity_bo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8305911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from botorch.test_functions.multi_fidelity import AugmentedHartmann\n",
    "\n",
    "# problem = AugmentedHartmann(negate=True).to()\n",
    "# fidelities = torch.tensor([0.5, 0.75, 1.0])\n",
    "\n",
    "# def generate_initial_data(n=16):\n",
    "#     # generate training data\n",
    "#     train_x = torch.rand(n, 6) # torch.Size([n, 6])\n",
    "#     train_f = fidelities[torch.randint(2, (n, 1))] # torch.Size([n, 1]), sampled fidelities of training data\n",
    "#     train_x_full = torch.cat((train_x, train_f), dim=1) # torch.Size([16, 7]), last col is associated fidelity\n",
    "#     train_obj = problem(train_x_full).unsqueeze(-1) # torch.Size([16, 1]), add output dimension\n",
    "#     return train_x_full, train_obj\n",
    "    \n",
    "\n",
    "# def initialize_model(train_x, train_obj):\n",
    "#     # define a surrogate model suited for a \"training data\"-like fidelity parameter\n",
    "#     # in dimension 6, as in [2]\n",
    "#     model = SingleTaskMultiFidelityGP(\n",
    "#         train_x, \n",
    "#         train_obj, \n",
    "#         outcome_transform=Standardize(m=1),\n",
    "#         data_fidelity=6\n",
    "#     )   \n",
    "#     mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "#     return mll, model\n",
    "\n",
    "# train_x, train_obj = generate_initial_data(n=16)\n",
    "# mll, model = initialize_model(train_x, train_obj)\n",
    "\n",
    "# fit_gpytorch_model(mll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2743e79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cumulative_cost = 0.0\n",
    "# N_ITER = 3 if not SMOKE_TEST else 1\n",
    "\n",
    "\n",
    "# for _ in range(N_ITER):\n",
    "#     mll, model = initialize_model(train_x, train_obj)\n",
    "#     fit_gpytorch_model(mll)\n",
    "#     mfkg_acqf = get_mfkg(model)\n",
    "#     new_x, new_obj, cost = optimize_mfkg_and_get_observation(mfkg_acqf)\n",
    "#     train_x = torch.cat([train_x, new_x])\n",
    "#     train_obj = torch.cat([train_obj, new_obj])\n",
    "#     cumulative_cost += cost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
