{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8613aa76",
   "metadata": {},
   "source": [
    "## System Description\n",
    "1. We have a set of COFs from a database. Each COF is characterized by a feature vector $$x_{COF} \\in X \\subset R^d$$ were d=14.\n",
    "\n",
    "\n",
    "2. We have **two different types** of simulations to calculate **the same material property $S_{Xe/Kr}$**. Therefore, we have a Single-Task/Objective (find the material with the optimal selevtivity), Multi-Fidelity problem. \n",
    "    1. low-fidelity  = Henry Coefficient calculation - MC integration - cost=1\n",
    "    2. high-fidelity = GCMC mixture simulation - 80:20 (Kr:Xe) at 298 K and 1.0 bar - cost=30\n",
    "\n",
    "\n",
    "3. We will initialize the system with *two* COFs at both fidelities in order to initialize the Covariance Matrix.\n",
    "    - The fist COF will be the one closest to the center of the normalized feature space\n",
    "    - The second COF will be chosen at random\n",
    "\n",
    "\n",
    "4. Each surrogate model will **only train on data acquired at its level of fidelity** (Heterotopic data). $$X_{lf} \\neq X_{hf} \\subset X$$\n",
    "    1. We are using the augmented EI acquisition function from [here](https://link.springer.com/content/pdf/10.1007/s00158-005-0587-0.pdf)\n",
    "\n",
    "\n",
    "5. **kernel model**: \n",
    "    1.  We need a Gaussian Process (GP) that will give a *correlated output for each fidelity* i.e. we need a vector-valued kernel\n",
    "    2. Given the *cost aware* acquisition function, which imposes a fidelity hierarchy, we anticipate the number of training points at each fidelity *will not* be equal (asymmetric scenario) $$n_{lf} > n_{hf}$$\n",
    "        - perhaps we can force the symmetric case, $n_{lf} = n_{hf} = n$, if we can include `missing` or `empty` entries in the training sets.\n",
    "\n",
    "\n",
    "Note: even though we have heterotopic data in an asymmetric scenario -- due to hierarchical, multi-fidelity -- we can still use a symmetric multi-output GP. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b61313",
   "metadata": {},
   "source": [
    "### Strategy\n",
    "1. Implement SingleTaskMultiFidelity Gp\n",
    "2. Get augmented EI working\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7679eb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gpytorch\n",
    "from botorch.models import SingleTaskMultiFidelityGP\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch import fit_gpytorch_model\n",
    "from scipy.stats import norm\n",
    "import math \n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fccba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.models.transforms.outcome import Standardize\n",
    "from botorch.utils.transforms import unnormalize, standardize\n",
    "from botorch.utils.sampling import draw_sobol_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d83c705d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data - \n",
      "X: torch.Size([608, 14])\n",
      "henry_y: torch.Size([608])\n",
      "gcmc_y:  torch.Size([608])\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "#  import data\n",
    "###\n",
    "f = h5py.File(\"targets_and_normalized_features.jld2\", \"r\")\n",
    "\n",
    "X = torch.from_numpy(np.transpose(f[\"X\"][:]))\n",
    "henry_y = torch.from_numpy(np.transpose(f[\"henry_y\"][:]))\n",
    "gcmc_y  = torch.from_numpy(np.transpose(f[\"gcmc_y\"][:]))\n",
    "print(\"data - \\nX:\", X.shape)\n",
    "print(\"henry_y:\", henry_y.shape)\n",
    "print(\"gcmc_y: \", gcmc_y.shape)\n",
    "\n",
    "###\n",
    "#  construct initial inputs\n",
    "#  1. get initial points\n",
    "#  2. standardize outputs\n",
    "#  3. stack into tensor\n",
    "###\n",
    "nb_COFs = henry_y.shape[0] # total number of COFs data points \n",
    "nb_COFs_initialization = 7 # number of COFs to initialize with\n",
    "ids_acquired = np.random.choice(np.arange((nb_COFs)), size=nb_COFs_initialization, replace=False)\n",
    "\n",
    "fidelities = torch.tensor([0.1, 1.0]) # assign fidelities (arbitrary?)\n",
    "fid_acquired = torch.randint(2, (nb_COFs_initialization, 1))\n",
    "train_f = fidelities[fid_acquired] # selected fidelity of training points\n",
    "\n",
    "train_x = X[ids_acquired, :] # torch.Size([nb_COFs_initialization, 14])\n",
    "train_x_full = torch.cat((train_x, train_f), dim=1) # last col is associated fidelity\n",
    "\n",
    "\n",
    "y = torch.tensor((), dtype=torch.float64).new_zeros((ids_acquired.shape[0], 1))\n",
    "for i, fid in enumerate(fid_acquired):\n",
    "    if fid == 0:\n",
    "        y[i][0] = henry_y[ids_acquired[i]]\n",
    "    else:\n",
    "        y[i][0] = gcmc_y[ids_acquired[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a420c8eb",
   "metadata": {},
   "source": [
    "**Construct Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3fb19d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.test_functions.multi_fidelity import AugmentedHartmann\n",
    "\n",
    "\n",
    "problem = AugmentedHartmann(negate=True).to()\n",
    "fidelities = torch.tensor([0.5, 0.75, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8305911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_initial_data(n=16):\n",
    "    # generate training data\n",
    "    train_x = torch.rand(n, 6) # torch.Size([n, 6])\n",
    "    train_f = fidelities[torch.randint(2, (n, 1))] # torch.Size([n, 1]), sampled fidelities of training data\n",
    "    train_x_full = torch.cat((train_x, train_f), dim=1) # torch.Size([16, 7]), last col is associated fidelity\n",
    "    train_obj = problem(train_x_full).unsqueeze(-1) # torch.Size([16, 1]), add output dimension\n",
    "    return train_x_full, train_obj\n",
    "    \n",
    "def initialize_model(train_x, train_obj):\n",
    "    # define a surrogate model suited for a \"training data\"-like fidelity parameter\n",
    "    # in dimension 6, as in [2]\n",
    "    model = SingleTaskMultiFidelityGP(\n",
    "        train_x, \n",
    "        train_obj, \n",
    "        outcome_transform=Standardize(m=1),\n",
    "        data_fidelity=6\n",
    "    )   \n",
    "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "    return mll, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbfd5f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_obj = generate_initial_data(n=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
