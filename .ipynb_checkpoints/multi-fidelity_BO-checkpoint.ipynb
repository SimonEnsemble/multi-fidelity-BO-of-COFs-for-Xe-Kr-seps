{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5af771ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "using JLD2\n",
    "using PyPlot\n",
    "using StatsBase # Statistics\n",
    "using Distributions\n",
    "\n",
    "# using ScikitLearn # machine learning package\n",
    "# @sk_import gaussian_process : GaussianProcessRegressor\n",
    "# @sk_import gaussian_process.kernels : Matern\n",
    "\n",
    "using PyCall\n",
    "# @pyimport torch\n",
    "@pyimport gpytorch\n",
    "@pyimport botorch\n",
    "\n",
    "# config plot settings\n",
    "PyPlot.matplotlib.style.use(\"ggplot\")\n",
    "rcParams = PyPlot.PyDict(PyPlot.matplotlib.\"rcParams\")\n",
    "rcParams[\"font.size\"] = 16;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dca76ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Symbol}:\n",
       " :X\n",
       " :henry_y\n",
       " :gcmc_y"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@load joinpath(pwd(), \"targets_and_normalized_features.jld2\") X henry_y gcmc_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f91aff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <class 'botorch.models.gp_regression.SingleTaskGP'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "botorch.models.SingleTaskGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f03525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct and fit GP model\n",
    "model = SingleTaskGP(X[ids_acquired, :], y_acquired)\n",
    "mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "fit_gpytorch_model(mll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d550916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from botorch.models import FixedNoiseGP, SingleTaskGP\n",
    "# from gpytorch.kernels import ScaleKernel\n",
    "# from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "# from botorch import fit_gpytorch_model\n",
    "# from scipy.stats import norm\n",
    "# from botorch.acquisition.analytic import ExpectedImprovement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29365770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject MultitaskGaussianLikelihood(\n",
       "  (raw_task_noises_constraint): GreaterThan(1.000E-04)\n",
       "  (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34f89e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = MultitaskGPModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f99f415",
   "metadata": {},
   "source": [
    "## Multi-fidelity BO Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405c223a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179e0859",
   "metadata": {},
   "outputs": [],
   "source": [
    "### procedure:\n",
    "# 1. select initial COF identifiers set to train GP \n",
    "# 2. initialize and normalize array of initial target data \n",
    "# 3. itterate through budgetted number of BO runs\n",
    "#    a. construct GP model with kernel\n",
    "#    b. fit GP to current data for acquired COFs => gives ŷ(x)\n",
    "#    c. construct acquiaition function A(x)\n",
    "#    d. determine which COF to acquire next => evaluate argmax(A(x))\n",
    "#       i. for EI track if this is an exploitation or an exploration\n",
    "#    e. append COF identified in step 3.d to list of acquired COFs \n",
    "# 4. update final set of acquired COF data and normalize\n",
    "# 5. return the IDs for the set of acquired COFs\n",
    "###\n",
    "\"\"\"\n",
    "# Arguments\n",
    "- `X`: feature matrix\n",
    "- `y_lf`: low-fidelity target vector\n",
    "- `y_hf`: high_fidelity target vector\n",
    "- `nb_iterations`: maximum number of BO iterations (experiment budget)\n",
    "- `which_acquisition`: which acquisition function to implement\n",
    "` `store_explore_exploit_terms`: whether or not to keep track of the explore and exploit \n",
    "                                 terms from the acqisition for the acquired material at each iteration\n",
    "- `sample_gp`: whether or not to store sample GP functions\n",
    "- `initialize_with`: specify which and/or how many materials to initialize the search\n",
    "- `kwargs`: dictionary of optional keyword arguments\n",
    "\"\"\"\n",
    "function run_bayesian_optimization(X, y_lf, y_hf, nb_iterations::Int, \n",
    "                                   nb_COFs_initialization::Int;\n",
    "                                   which_acquisition::Symbol=:EI,\n",
    "                                   store_explore_exploit_terms::Bool=false,\n",
    "                                   initialize_with::Union{Array{Int, 1}, Nothing}=nothing,\n",
    "                                   kwargs::Dict{Symbol, Any}=Dict{Symbol, Any}())\n",
    "    # quick checks\n",
    "    @assert nb_iterations > nb_COFs_initialization \"More initializations than itterations not allowed.\"\n",
    "    @assert which_acquisition in [:UCB] \"Acquisition function not supported:\\t $(which_acquisition)\"\n",
    "    \n",
    "    # create array to store explore-explot terms if needed\n",
    "    if store_explore_exploit_terms\n",
    "        explore_exploit_balance = []\n",
    "    end\n",
    "    \n",
    "    ###\n",
    "    #  initialize array to track fidelities:\n",
    "    #    low fidelity  => 0\n",
    "    #    high fidelity => 1\n",
    "    ###\n",
    "    fidelity_query = zeros(nb_iterations)\n",
    "    \n",
    "    # initialize Normal Distribution for EI\n",
    "    normal = Normal()\n",
    "    \n",
    "    ###\n",
    "    #  1. randomly select COF IDs for training initial GP\n",
    "    #     or use COFs passes into function\n",
    "    ###\n",
    "    if isnothing(initialize_with)\n",
    "        ids_acquired = StatsBase.sample(1:nb_COFs, nb_COFs_initialization, replace=false)\n",
    "        @assert length(unique(ids_acquired)) == nb_COFs_initialization\n",
    "    else\n",
    "        # initialize using a specified set of indecies\n",
    "        ids_acquired = initialize_with\n",
    "        @assert length(unique(ids_acquired)) == nb_COFs_initialization\n",
    "    end\n",
    "     \n",
    "    ###\n",
    "    #  3. itterate through budgetted number of BO runs\n",
    "    ###\n",
    "    for i in range(nb_COFs_initialization, stop=nb_iterations)\n",
    "        ###\n",
    "        #  a-b. construct and fit GP model for both fidelities\n",
    "        ###\n",
    "        kernel = Matern(nu=2.5, length_scale=0.25) \n",
    "        model = GaussianProcessRegressor(kernel=kernel, normalize_y=true, n_restarts_optimizer=5)\n",
    "        model.fit(X[ids_acquired, :], y[ids_acquired])\n",
    "        \n",
    "        kernel_hf = Matern(nu=2.5, length_scale=0.25) \n",
    "        model_hf = GaussianProcessRegressor(kernel=kernel, normalize_y=true, n_restarts_optimizer=5)\n",
    "#         model_hf.fit(X[ids_acquired, :], y[ids_acquired])\n",
    "        \n",
    "        if sample_gp # Currently not working\n",
    "            sample_y = model.sample_y(X)\n",
    "            push!(store_sample_y, sample_y)\n",
    "        end\n",
    "        \n",
    "        ŷ_lf, σ_lf = model_lf.predict(X, return_std=true)\n",
    "        \n",
    "        ###\n",
    "        #  c. setup acquisition function\n",
    "        ###\n",
    "\n",
    "        \n",
    "        ###\n",
    "        #  d. determine which COF to acquire next\n",
    "        #     and with which fidelity\n",
    "        ###\n",
    "        ids_sorted_by_aquisition = sortperm(acquisition_values, rev=true)\n",
    "        for id_max_aquisition in ids_sorted_by_aquisition\n",
    "            if ! (id_max_aquisition in ids_acquired)\n",
    "                ###\n",
    "                #  e. acqurie this COF (i.e. update list)\n",
    "                ###\n",
    "                push!(ids_acquired, id_max_aquisition)\n",
    "                \n",
    "                ###\n",
    "                #  Determine at which Fidelity the COF will be evaluated\n",
    "                ###\n",
    "                fidelity_query[i] = 0 # low  fidelity\n",
    "                fidelity_query[i] = 1 # high fidelity\n",
    "                \n",
    "                if store_explore_exploit_terms\n",
    "                    # store explore and exploit terms\n",
    "                    push!(explore_exploit_balance, \n",
    "                          [exploit_term[id_max_aquisition], explore_term[id_max_aquisition]])\n",
    "                end\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "        # quick check\n",
    "        @assert length(ids_acquired) == i + 1\n",
    "    end\n",
    "    \n",
    "    # quick check (remember to account for final COF to be acquired)\n",
    "    @assert length(ids_acquired) == nb_iterations + 1 \"length(ids_acquired) = $(length(ids_acquired))\"\n",
    "    \n",
    "    ###\n",
    "    #  5. return the IDs for the set of acquired COFs\n",
    "    ###\n",
    "    return ids_acquired, explore_exploit_balance\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
