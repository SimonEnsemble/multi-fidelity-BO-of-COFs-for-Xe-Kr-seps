{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f7fef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "using JLD2\n",
    "using PyPlot\n",
    "using StatsBase # Statistics\n",
    "using Distributions\n",
    "\n",
    "using PyCall\n",
    "@pyimport torch \n",
    "@pyimport gpytorch\n",
    "@pyimport botorch\n",
    "\n",
    "\n",
    "# config plot settings\n",
    "PyPlot.matplotlib.style.use(\"ggplot\")\n",
    "rcParams = PyPlot.PyDict(PyPlot.matplotlib.\"rcParams\")\n",
    "rcParams[\"font.size\"] = 16;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "803e9004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Symbol}:\n",
       " :X\n",
       " :henry_y\n",
       " :gcmc_y"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###\n",
    "#  load data and construct input tensors\n",
    "###\n",
    "@load joinpath(pwd(), \"targets_and_normalized_features.jld2\") X henry_y gcmc_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0af5b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Arguments\n",
    "- `X`: feature matrix\n",
    "- `y`: target vector\n",
    "- `nb_iterations`: maximum number of BO iterations (experiment budget)\n",
    "- `which_acquisition`: which acquisition function to implement\n",
    "` `store_explore_exploit_terms`: whether or not to keep track of the explore and exploit \n",
    "                                 terms from the acqisition for the acquired material at each iteration\n",
    "- `sample_gp`: whether or not to store sample GP functions\n",
    "- `initialize_with`: specify which and/or how many materials to initialize the search\n",
    "- `kwargs`: dictionary of optional keyword arguments\n",
    "\"\"\"\n",
    "function run_bayesian_optimization(X, y1, y2, nb_iterations::Int, \n",
    "                                   nb_COFs_initialization::Int;\n",
    "                                   which_acquisition::Symbol=:EI,\n",
    "                                   store_explore_exploit_terms::Bool=false,\n",
    "                                   sample_gp::Bool=false,\n",
    "                                   initialize_with::Union{Array{Int, 1}, Nothing}=nothing,\n",
    "                                   kwargs::Dict{Symbol, Any}=Dict{Symbol, Any}())\n",
    "    # quick checks\n",
    "    @assert nb_iterations > nb_COFs_initialization \"More initializations than itterations not allowed.\"\n",
    "    @assert which_acquisition in [:EI] \"Acquisition function not supported:\\t $(which_acquisition)\"\n",
    "    \n",
    "    # create array to store explore-explot terms if needed\n",
    "    if store_explore_exploit_terms\n",
    "        # store as (explore, exploit, fidelity)\n",
    "        explore_exploit_balance = Tuple{Float64, Float64, Int64}[]\n",
    "    end\n",
    "    \n",
    "    ###\n",
    "    #  1. randomly select COF IDs for training initial GP\n",
    "    ###\n",
    "    if isnothing(initialize_with)\n",
    "        ids_acquired = StatsBase.sample(1:nb_COFs, nb_COFs_initialization, replace=false)\n",
    "        @assert length(unique(ids_acquired)) == nb_COFs_initialization\n",
    "    else\n",
    "        # initialize using a specified set of indecies\n",
    "        ids_acquired = initialize_with\n",
    "        fidelity = 1\n",
    "        @assert length(unique(ids_acquired)) == nb_COFs_initialization\n",
    "    end\n",
    "    # initialize using ONLY the high-fidelity results\n",
    "    x = X[ids_acquired, :]\n",
    "    train_X = torch.from_numpy(x)       # feature vectors\n",
    "    y1 = torch.from_numpy(gcmc_y[ids_acquired])  # low-fidelity\n",
    "    y2 = torch.from_numpy(gcmc_y[ids_acquired])  # high-fidelity\n",
    "    train_y = torch.stack([y1, y2], -1)\n",
    "    # standardize outputs using *only currently acquired data*\n",
    "    y_acquired = (y_acquired - torch.mean(y_acquired)) / torch.std(y_acquired)\n",
    "    \n",
    "    # uses ICM [here](https://botorch.org/api/models.html#multitaskgp)\n",
    "    # botorch.models.multitask.MultiTaskGP(train_X, train_Y, task_feature, \n",
    "    #     task_covar_prior=None, output_tasks=None, \n",
    "    #     rank=None, input_transform=None, outcome_transform=None)\n",
    "    model = botorch.models.multitask.MultiTaskGP(train_X, train_Y, -1)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b7c373",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7bc93be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject tensor([[ 1.1141,  1.1141],\n",
       "        [-0.5547, -0.5547],\n",
       "        [ 0.9686,  0.9686],\n",
       "        [-1.4142, -1.4142],\n",
       "        [-0.1138, -0.1138]], dtype=torch.float64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_acquired = StatsBase.sample(1:600, 5, replace=false)\n",
    "\n",
    "x = X[ids_acquired, :]\n",
    "train_X = torch.from_numpy(x)       # feature vectors\n",
    "y1 = torch.from_numpy(gcmc_y[ids_acquired])  # low-fidelity\n",
    "y2 = torch.from_numpy(gcmc_y[ids_acquired])  # high-fidelity\n",
    "y_acquired = torch.stack([y1, y2], -1)\n",
    "# standardize outputs using *only currently acquired data*\n",
    "y_acquired = (y_acquired - torch.mean(y_acquired)) / torch.std(y_acquired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "115cfb01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject MultiTaskGP(\n",
       "  (likelihood): GaussianLikelihood(\n",
       "    (noise_covar): HomoskedasticNoise(\n",
       "      (noise_prior): GammaPrior()\n",
       "      (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "    )\n",
       "  )\n",
       "  (mean_module): ConstantMean()\n",
       "  (covar_module): ScaleKernel(\n",
       "    (base_kernel): MaternKernel(\n",
       "      (lengthscale_prior): GammaPrior()\n",
       "      (raw_lengthscale_constraint): Positive()\n",
       "    )\n",
       "    (outputscale_prior): GammaPrior()\n",
       "    (raw_outputscale_constraint): Positive()\n",
       "  )\n",
       "  (task_covar_module): IndexKernel(\n",
       "    (raw_var_constraint): Positive()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = botorch.models.multitask.MultiTaskGP(train_X, y_acquired, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67c43a6",
   "metadata": {},
   "source": [
    "## RUN MTBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a666ec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###\n",
    "# #  randomly select points and fidelities to initialize with\n",
    "# ###\n",
    "# nb_COFs_initialization = 1\n",
    "# ids_acquired = []\n",
    "# for i in 1:nb_COFs_initialization\n",
    "#     # index of test sample \n",
    "#     ind = StatsBase.sample(1:length(henry_y), 1, replace=false)\n",
    "#     # fidelity of test\n",
    "#     fid =  StatsBase.sample(1:2, 1, replace=false)\n",
    "#     push!(ids_acquired, (ind[1], fid[1]))\n",
    "# end\n",
    "    \n",
    "# if ! (in((26, 1), ids_acquired))\n",
    "#     pushfirst!(ids_acquired, (26, 1))\n",
    "# end\n",
    "# ids_acquired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142bd2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_bayesian_optimization(X, y1, y2, nb_iterations::Int, \n",
    "#                            nb_COFs_initialization::Int;\n",
    "#                            which_acquisition::Symbol=:EI,\n",
    "#                            store_explore_exploit_terms::Bool=false,\n",
    "#                            sample_gp::Bool=false,\n",
    "#                            initialize_with::Union{Array{Int, 1}, Nothing}=nothing,\n",
    "#                            kwargs::Dict{Symbol, Any}=Dict{Symbol, Any}())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b233c55f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba8d712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultitaskGPModel(gpytorch.models.ExactGP):\n",
    "#     def __init__(self, train_x, train_y, likelihood):\n",
    "#         super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "#         self.mean_module = gpytorch.means.MultitaskMean(\n",
    "#             gpytorch.means.ConstantMean(), num_tasks=2\n",
    "#         )\n",
    "#         self.covar_module = gpytorch.kernels.MultitaskKernel(\n",
    "#             gpytorch.kernels.RBFKernel(), num_tasks=2, rank=1\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         mean_x = self.mean_module(x)\n",
    "#         covar_x = self.covar_module(x)\n",
    "#         return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "# likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=2)\n",
    "# model = MultitaskGPModel(train_x, train_y, likelihood)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.2",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
