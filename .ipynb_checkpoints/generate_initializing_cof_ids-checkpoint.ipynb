{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0a0ad8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import h5py # for .jld2 files\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2ac2959",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw data - \n",
      "\tX: torch.Size([608, 14])\n",
      "\tfidelity: 0\n",
      "\t\ty: torch.Size([608])\n",
      "\t\tcost:  (608,)\n",
      "\tfidelity: 1\n",
      "\t\ty: torch.Size([608])\n",
      "\t\tcost:  (608,)\n",
      "\n",
      "Ensure features are normalized - \n",
      "max:\n",
      " tensor([5.6399e+01, 9.2784e-01, 6.3570e+03, 1.6107e+03, 1.8182e-01, 2.5000e-01,\n",
      "        6.6667e-01, 5.0000e-01, 2.9520e-02, 3.3333e-01, 1.4286e-01, 6.6667e-02,\n",
      "        2.8571e-01, 2.3810e-02], dtype=torch.float64)\n",
      "min:\n",
      " tensor([3.5094e+00, 1.6356e-01, 1.9966e+03, 1.0272e+02, 0.0000e+00, 0.0000e+00,\n",
      "        3.2500e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00], dtype=torch.float64)\n",
      "width:\n",
      " tensor([5.2889e+01, 7.6428e-01, 4.3604e+03, 1.5080e+03, 1.8182e-01, 2.5000e-01,\n",
      "        3.4167e-01, 5.0000e-01, 2.9520e-02, 3.3333e-01, 1.4286e-01, 6.6667e-02,\n",
      "        2.8571e-01, 2.3810e-02], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "#  Load Data\n",
    "###\n",
    "# file = h5py.File(\"targets_and_normalized_features.jld2\", \"r\")\n",
    "file = h5py.File(\"targets_and_standardized_features.jld2\", \"r\")\n",
    "\n",
    "# feature matrix\n",
    "X = torch.from_numpy(np.transpose(file[\"X\"][:]))\n",
    "# simulation data\n",
    "y = [torch.from_numpy(np.transpose(file[\"henry_y\"][:])), \n",
    "     torch.from_numpy(np.transpose(file[\"gcmc_y\"][:]))]\n",
    "# associated simulation costs\n",
    "cost = [np.transpose(file[\"henry_total_elapsed_time\"][:]), \n",
    "        np.transpose(file[\"gcmc_elapsed_time\"][:])]\n",
    "\n",
    "# total number of COFs in data set\n",
    "nb_COFs = X.shape[0] \n",
    "\n",
    "print(\"raw data - \\n\\tX:\", X.shape)\n",
    "for f in range(2):\n",
    "    print(\"\\tfidelity:\", f)\n",
    "    print(\"\\t\\ty:\", y[f].shape)\n",
    "    print(\"\\t\\tcost: \", cost[f].shape)\n",
    "    \n",
    "print(\"\\nEnsure features are normalized - \")\n",
    "print(\"max:\\n\", torch.max(X, 0).values)\n",
    "print(\"min:\\n\", torch.min(X, 0).values)\n",
    "print(\"width:\\n\",torch.max(X, 0).values - torch.min(X, 0).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ec0918e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#  helper functions\n",
    "###\n",
    "\n",
    "# find COF closest to the center of data space\n",
    "def get_initializing_COF(X):\n",
    "    # center of feature space\n",
    "    data_center = np.array([X[:, i].mean() for i in range(X.size()[1])])\n",
    "    # min distance to center \n",
    "    return np.argmin(np.linalg.norm(X - data_center, axis=1))\n",
    "\n",
    "# find COFs farthest away from a specified point\n",
    "def diverse_set(X, seed_cof, train_size):\n",
    "    # initialize with one random point; pick others in a max diverse fashion\n",
    "    ids_train = [seed_cof]\n",
    "    # select remaining training points\n",
    "    for j in range(train_size - 1):\n",
    "        # for each point in data set, compute its min dist to training set\n",
    "        dist_to_train_set = np.linalg.norm(X - X[ids_train, None, :], axis=2)\n",
    "        assert np.shape(dist_to_train_set) == (len(ids_train), nb_COFs)\n",
    "        min_dist_to_a_training_pt = np.min(dist_to_train_set, axis=0)\n",
    "        assert np.size(min_dist_to_a_training_pt) == nb_COFs\n",
    "        \n",
    "        # acquire point with max(min distance to train set) i.e. Furthest from train set\n",
    "        ids_train.append(np.argmax(min_dist_to_a_training_pt))\n",
    "    assert np.size(np.unique(ids_train)) == train_size # must be unique\n",
    "    return np.array(ids_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fc1aebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([354,  45, 426]),\n",
       " array([236,  45, 588]),\n",
       " array([315,  45, 426]),\n",
       " array([353, 426,  45]),\n",
       " array([142,  45,  70]),\n",
       " array([294, 426, 583]),\n",
       " array([577,  45, 598]),\n",
       " array([512,  45, 426]),\n",
       " array([347,  45, 134]),\n",
       " array([342, 426,  45]),\n",
       " array([ 10, 426,  45]),\n",
       " array([485,  45, 426]),\n",
       " array([183, 426,  45]),\n",
       " array([203,  45, 426]),\n",
       " array([202,  45, 426])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###\n",
    "#  number of initializing sets to generate\n",
    "###\n",
    "nb_runs = 100\n",
    "\n",
    "###\n",
    "#  number of COFs in each initializing set\n",
    "###\n",
    "nb_init = 3\n",
    "\n",
    "###\n",
    "#  list of COF IDs to sample\n",
    "###\n",
    "cof_ids_to_sample = list(range(nb_COFs))\n",
    "\n",
    "###\n",
    "#  identify the COF at the center of data space\n",
    "###\n",
    "central_cof = get_initializing_COF(X)\n",
    "# remove this COF ID from sample set \n",
    "cof_ids_to_sample.pop(central_cof)\n",
    "\n",
    "###\n",
    "#  randomly select the rest without replacement\n",
    "###\n",
    "seed_cofs = random.sample(cof_ids_to_sample, nb_runs-1)\n",
    "\n",
    "\n",
    "###\n",
    "#  generate initializing sets using max diversity \n",
    "###\n",
    "init_cof_ids = []\n",
    "\n",
    "for i in range(nb_runs):\n",
    "    if i == 0:\n",
    "        cof_id = central_cof\n",
    "    else:\n",
    "        cof_id = seed_cofs[i-1]\n",
    "    # get diverse set\n",
    "    div_set = diverse_set(X, cof_id, nb_init)\n",
    "    init_cof_ids.append(div_set)\n",
    "    \n",
    "    \n",
    "init_cof_ids[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a19c74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#  save data\n",
    "###\n",
    "initializing_cof_ids = dict({'init_cof_ids': init_cof_ids\n",
    "                })\n",
    "\n",
    "with open('search_results/initializing_cof_ids.pkl', 'wb') as file:\n",
    "    pickle.dump(initializing_cof_ids, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
