{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0a0ad8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import h5py # for .jld2 files\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2ac2959",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw data - \n",
      "\tX: torch.Size([608, 14])\n",
      "\tfidelity: 0\n",
      "\t\ty: torch.Size([608])\n",
      "\t\tcost:  (608,)\n",
      "\tfidelity: 1\n",
      "\t\ty: torch.Size([608])\n",
      "\t\tcost:  (608,)\n",
      "\n",
      "Ensure features are normalized - \n",
      "max:\n",
      " tensor([0.7144, 0.4136, 0.4696, 0.6677, 0.9579, 0.8383, 0.3595, 0.3207, 0.9938,\n",
      "        0.8242, 0.9692, 0.9869, 0.9868, 0.9762], dtype=torch.float64)\n",
      "min:\n",
      " tensor([-0.2856, -0.5864, -0.5304, -0.3323, -0.0421, -0.1617, -0.6405, -0.6793,\n",
      "        -0.0062, -0.1758, -0.0308, -0.0131, -0.0132, -0.0238],\n",
      "       dtype=torch.float64)\n",
      "width:\n",
      " tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "#  Load Data\n",
    "###\n",
    "file = h5py.File(\"targets_and_normalized_features.jld2\", \"r\")\n",
    "\n",
    "# feature matrix\n",
    "X = torch.from_numpy(np.transpose(file[\"X\"][:]))\n",
    "# simulation data\n",
    "y = [torch.from_numpy(np.transpose(file[\"henry_y\"][:])), \n",
    "     torch.from_numpy(np.transpose(file[\"gcmc_y\"][:]))]\n",
    "# associated simulation costs\n",
    "cost = [np.transpose(file[\"henry_total_elapsed_time\"][:]), \n",
    "        np.transpose(file[\"gcmc_elapsed_time\"][:])]\n",
    "\n",
    "# total number of COFs in data set\n",
    "nb_COFs = X.shape[0] \n",
    "\n",
    "print(\"raw data - \\n\\tX:\", X.shape)\n",
    "for f in range(2):\n",
    "    print(\"\\tfidelity:\", f)\n",
    "    print(\"\\t\\ty:\", y[f].shape)\n",
    "    print(\"\\t\\tcost: \", cost[f].shape)\n",
    "    \n",
    "print(\"\\nEnsure features are normalized - \")\n",
    "print(\"max:\\n\", torch.max(X, 0).values)\n",
    "print(\"min:\\n\", torch.min(X, 0).values)\n",
    "print(\"width:\\n\",torch.max(X, 0).values - torch.min(X, 0).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ec0918e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#  helper functions\n",
    "###\n",
    "\n",
    "# find COF closest to the center of feature space\n",
    "def get_initializing_COF(X):\n",
    "    # center of feature space\n",
    "    data_center = np.array([X[:, i].mean() for i in range(X.size()[1])])\n",
    "    # max possible distance between normalized features\n",
    "    return np.argmin(np.linalg.norm(X - data_center, axis=1))\n",
    "\n",
    "# find COFs farthest away from a specified point\n",
    "def diverse_set(X, seed_cof, train_size):\n",
    "    # initialize with one random point; pick others in a max diverse fashion\n",
    "    ids_train = [seed_cof]\n",
    "    # select remaining training points\n",
    "    for j in range(train_size - 1):\n",
    "        # for each point in data set, compute its min dist to training set\n",
    "        dist_to_train_set = np.linalg.norm(X - X[ids_train, None, :], axis=2)\n",
    "        assert np.shape(dist_to_train_set) == (len(ids_train), nb_COFs)\n",
    "        min_dist_to_a_training_pt = np.min(dist_to_train_set, axis=0)\n",
    "        assert np.size(min_dist_to_a_training_pt) == nb_COFs\n",
    "        \n",
    "        # acquire point with max(min distance to train set) i.e. Furthest from train set\n",
    "        ids_train.append(np.argmax(min_dist_to_a_training_pt))\n",
    "    assert np.size(np.unique(ids_train)) == train_size # must be unique\n",
    "    return np.array(ids_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3da86f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([112, 522,  45]),\n",
       " array([508,  45,  73]),\n",
       " array([237, 176,  73]),\n",
       " array([308,  45,  73]),\n",
       " array([539,  71, 522]),\n",
       " array([411, 522,  73]),\n",
       " array([ 38,  45, 522]),\n",
       " array([ 71, 494,  98]),\n",
       " array([80, 73, 45]),\n",
       " array([259, 522,  73]),\n",
       " array([172,  98, 522]),\n",
       " array([423, 522,  45]),\n",
       " array([ 74,  45, 522]),\n",
       " array([ 89,  45, 522]),\n",
       " array([575, 522,  45])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###\n",
    "#  number of initializing sets to generate\n",
    "###\n",
    "nb_runs = 100\n",
    "\n",
    "###\n",
    "#  number of COFs in each initializing set\n",
    "###\n",
    "nb_init = 3\n",
    "\n",
    "###\n",
    "#  list of COF IDs to sample\n",
    "###\n",
    "cof_ids_to_sample = list(range(nb_COFs))\n",
    "\n",
    "###\n",
    "#  identify the COF at the center of data space\n",
    "###\n",
    "central_cof = get_initializing_COF(X)\n",
    "# remove this COF ID from sample set \n",
    "cof_ids_to_sample.pop(central_cof)\n",
    "\n",
    "###\n",
    "#  randomly select the rest without replacement\n",
    "###\n",
    "seed_cofs = random.sample(cof_ids_to_sample, nb_runs-1)\n",
    "\n",
    "\n",
    "###\n",
    "#  generate initializing sets using max diversity \n",
    "###\n",
    "init_cof_ids = []\n",
    "\n",
    "for i in range(nb_runs):\n",
    "    if i == 0:\n",
    "        cof_id = central_cof\n",
    "    else:\n",
    "        cof_id = seed_cofs[i-1]\n",
    "    # get diverse set\n",
    "    div_set = diverse_set(X, cof_id, nb_init)\n",
    "    init_cof_ids.append(div_set)\n",
    "    \n",
    "init_cof_ids[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a19c74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "#  save data\n",
    "###\n",
    "initializing_cof_ids = dict({'init_cof_ids': init_cof_ids\n",
    "                })\n",
    "\n",
    "with open('search_results/initializing_cof_ids.pkl', 'wb') as file:\n",
    "    pickle.dump(initializing_cof_ids, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
